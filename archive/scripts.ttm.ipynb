{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTM\n",
    "*Attempt 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from tsfm_public import TimeSeriesPreprocessor, get_datasets\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "import torch.nn as nn\n",
    "from torch import round, no_grad, FloatTensor\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 13\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ttm_data(data, context_length=512, forecast_length=96):\n",
    "    # Convert JSON data to DataFrame format\n",
    "    processed_data = []\n",
    "    \n",
    "    for plan_data in data:\n",
    "        # Extract state sequences\n",
    "        states = plan_data['plan']\n",
    "        \n",
    "        # Create a row for each state in the plan\n",
    "        for state in states:\n",
    "            row = {\n",
    "                'timestamp': len(processed_data),  # Use index as timestamp\n",
    "                **state  # Unpack state variables\n",
    "            }\n",
    "            processed_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Define column specifications for TTM\n",
    "    column_specifiers = {\n",
    "        \"timestamp_column\": \"timestamp\",\n",
    "        \"id_columns\": [],  # No separate time series IDs\n",
    "        \"target_columns\": [\"V1\", \"V21\", \"V22\", \"V31\", \"V32\"],\n",
    "        \"control_columns\": []\n",
    "    }\n",
    "    \n",
    "    # Split configuration\n",
    "    total_length = len(df)\n",
    "    split_config = {\n",
    "        \"train\": [0, int(0.7 * total_length)],\n",
    "        \"valid\": [int(0.7 * total_length), int(0.85 * total_length)],\n",
    "        \"test\": [int(0.85 * total_length), total_length]\n",
    "    }\n",
    "    \n",
    "    # Create TTM preprocessor\n",
    "    tsp = TimeSeriesPreprocessor(\n",
    "        **column_specifiers,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_length,\n",
    "        scaling=True,\n",
    "        encode_categorical=False,\n",
    "        scaler_type=\"standard\"\n",
    "    )\n",
    "    \n",
    "    return get_datasets(tsp, df, split_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ttm_planner(context_length=512, forecast_length=96):    \n",
    "    # Get base TTM model\n",
    "    model = get_model(\n",
    "        \"ibm-granite/granite-timeseries-ttm-r2\",\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_length,\n",
    "        head_dropout=0.3\n",
    "    )\n",
    "    \n",
    "    # Modify for discrete state prediction\n",
    "    # Add discretization layer or post-processing\n",
    "    class TTMPlanner(nn.Module):\n",
    "        def __init__(self, base_model):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.discretize = nn.Linear(forecast_length, 5)  # 5 state variables\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Get TTM predictions\n",
    "            base_output = self.base_model(x)\n",
    "            # Discretize outputs to valid state values\n",
    "            discrete_states = round(self.discretize(base_output))\n",
    "            return discrete_states\n",
    "            \n",
    "    return TTMPlanner(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plan(model, initial_state, goal_state):\n",
    "    current_state = list(initial_state.values())\n",
    "    goal = list(goal_state.values())\n",
    "    plan = [initial_state]\n",
    "    \n",
    "    # Generate plan steps until we reach goal or max steps\n",
    "    max_steps = 10\n",
    "    for _ in range(max_steps):\n",
    "        # Prepare input - concatenate current state and goal\n",
    "        input_vector = np.array(current_state + goal)  # Convert to numpy array first\n",
    "        x = FloatTensor(input_vector).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Get model prediction\n",
    "        with no_grad():\n",
    "            next_state = model(x).numpy()[0]\n",
    "            \n",
    "        # Round to nearest valid state values\n",
    "        next_state = np.round(next_state).astype(int)\n",
    "        \n",
    "        # Convert to dict format\n",
    "        next_state_dict = {\n",
    "            'V1': next_state[0],\n",
    "            'V21': next_state[1],\n",
    "            'V22': next_state[2],\n",
    "            'V31': next_state[3],\n",
    "            'V32': next_state[4]\n",
    "        }\n",
    "        \n",
    "        plan.append(next_state_dict)\n",
    "        \n",
    "        # Check if we reached goal\n",
    "        if np.array_equal(next_state, goal):\n",
    "            break\n",
    "            \n",
    "        current_state = next_state.tolist()  # Convert numpy array to list for next iteration\n",
    "    \n",
    "    return plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_state_validity(state):\n",
    "    \"\"\"\n",
    "    Check if a state follows all Blocks World constraints based on the encoding:\n",
    "    V1: What's on table (0=None, 1=A, 2=B, 3=A,B)\n",
    "    V21: What's below A (0=None, 1=B, 2=Table)\n",
    "    V22: What's on top of A (0=None, 1=B)\n",
    "    V31: What's below B (0=None, 1=A, 2=Table)\n",
    "    V32: What's on top of B (0=None, 1=A)\n",
    "    \"\"\"\n",
    "    # Initialize validity flags\n",
    "    valid = True\n",
    "    reasons = []\n",
    "\n",
    "    # 1. Range checks\n",
    "    if not (0 <= state['V1'] <= 3):\n",
    "        valid = False\n",
    "        reasons.append(\"V1 out of range [0-3]\")\n",
    "    if not (0 <= state['V21'] <= 2):\n",
    "        valid = False\n",
    "        reasons.append(\"V21 out of range [0-2]\")\n",
    "    if not (0 <= state['V22'] <= 1):\n",
    "        valid = False\n",
    "        reasons.append(\"V22 out of range [0-1]\")\n",
    "    if not (0 <= state['V31'] <= 2):\n",
    "        valid = False\n",
    "        reasons.append(\"V31 out of range [0-2]\")\n",
    "    if not (0 <= state['V32'] <= 1):\n",
    "        valid = False\n",
    "        reasons.append(\"V32 out of range [0-1]\")\n",
    "\n",
    "    # 2. Table consistency (V1)\n",
    "    # If A is on table (V21=2), V1 should include A (V1=1 or V1=3)\n",
    "    if state['V21'] == 2 and state['V1'] not in [1, 3]:\n",
    "        valid = False\n",
    "        reasons.append(\"A on table but V1 doesn't reflect this\")\n",
    "    \n",
    "    # If B is on table (V31=2), V1 should include B (V1=2 or V1=3)\n",
    "    if state['V31'] == 2 and state['V1'] not in [2, 3]:\n",
    "        valid = False\n",
    "        reasons.append(\"B on table but V1 doesn't reflect this\")\n",
    "\n",
    "    # 3. Block A position consistency\n",
    "    # If A is on B (V21=1), then B must have A on top (V32=1)\n",
    "    if state['V21'] == 1 and state['V32'] != 1:\n",
    "        valid = False\n",
    "        reasons.append(\"A is on B but B doesn't have A on top\")\n",
    "\n",
    "    # 4. Block B position consistency\n",
    "    # If B is on A (V31=1), then A must have B on top (V22=1)\n",
    "    if state['V31'] == 1 and state['V22'] != 1:\n",
    "        valid = False\n",
    "        reasons.append(\"B is on A but A doesn't have B on top\")\n",
    "\n",
    "    # 5. Single position constraints\n",
    "    # A block can't be in multiple positions simultaneously\n",
    "    if state['V21'] == 1 and state['V21'] == 2:  # A can't be both on B and table\n",
    "        valid = False\n",
    "        reasons.append(\"A can't be both on B and table\")\n",
    "    if state['V31'] == 1 and state['V31'] == 2:  # B can't be both on A and table\n",
    "        valid = False\n",
    "        reasons.append(\"B can't be both on A and table\")\n",
    "\n",
    "    # 6. Mutual exclusion\n",
    "    # A and B can't be on top of each other simultaneously\n",
    "    if state['V22'] == 1 and state['V32'] == 1:\n",
    "        valid = False\n",
    "        reasons.append(\"A and B can't be on top of each other simultaneously\")\n",
    "\n",
    "    # 7. Support consistency\n",
    "    # If nothing is below A (V21=0), it can't have anything on top (V22=0)\n",
    "    if state['V21'] == 0 and state['V22'] != 0:\n",
    "        valid = False\n",
    "        reasons.append(\"A has no support but has something on top\")\n",
    "    \n",
    "    # If nothing is below B (V31=0), it can't have anything on top (V32=0)\n",
    "    if state['V31'] == 0 and state['V32'] != 0:\n",
    "        valid = False\n",
    "        reasons.append(\"B has no support but has something on top\")\n",
    "\n",
    "    return {\n",
    "        'valid': valid,\n",
    "        'reasons': reasons if not valid else [\"State is valid\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_plan_validity(predicted_plan):\n",
    "    \"\"\"Evaluate if each state transition is valid\"\"\"\n",
    "    valid_states = sum(check_state_validity(state)['valid'] for state in predicted_plan)\n",
    "    return {\n",
    "        'valid_states_ratio': valid_states / len(predicted_plan),\n",
    "        'completely_valid': valid_states == len(predicted_plan)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_goal_achievement(predicted_plan, goal_state):\n",
    "    \"\"\"Check if plan reaches the goal state\"\"\"\n",
    "    final_state = predicted_plan[-1]\n",
    "    goal_reached = all(final_state[k] == goal_state[k] for k in goal_state)\n",
    "    \n",
    "    if goal_reached:\n",
    "        steps_to_goal = len(predicted_plan) - 1\n",
    "    else:\n",
    "        steps_to_goal = float('inf')\n",
    "        \n",
    "    return {\n",
    "        'goal_reached': goal_reached,\n",
    "        'steps_to_goal': steps_to_goal,\n",
    "        # 'final_state_similarity': sum(final_state[k] == goal_state[k]  for k in goal_state) / len(goal_state)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model, test_data):\n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_data:\n",
    "        initial_state = test_case['initial_state']\n",
    "        goal_state = test_case['goal_state']\n",
    "        reference_plan = test_case['plan']\n",
    "        \n",
    "        # Generate plan using model\n",
    "        predicted_plan = generate_plan(model, initial_state, goal_state)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        goal_achievement = evaluate_goal_achievement(predicted_plan, goal_state)\n",
    "        plan_validation = evaluate_plan_validity(predicted_plan)\n",
    "        \n",
    "        results.append({\n",
    "            'goal_reached': goal_achievement['goal_reached'],  # Bool\n",
    "            'steps_to_goal': goal_achievement['steps_to_goal'],  # Float\n",
    "            'valid_states_ratio': plan_validation['valid_states_ratio'],  # Float\n",
    "            'completely_valid': plan_validation['completely_valid'],  # Bool\n",
    "            'reference_length': len(reference_plan),  # Float\n",
    "        })\n",
    "    \n",
    "    # Calculate aggregate metrics for all results\n",
    "    return {\n",
    "        \"success_rate\" : sum(r['goal_reached'] for r in results) / len(results),\n",
    "        \"avg_steps_to_goal\" : np.mean([r['steps_to_goal'] for r in results]),\n",
    "        \"avg_valid_states_ratio\" : np.mean([r['valid_states_ratio'] for r in results]),\n",
    "        \"valid_rate\" : sum(r['completely_valid'] for r in results) / len(results),\n",
    "        \"avg_reference_length\" : np.mean([r['reference_length'] for r in results]),\n",
    "        'detailed_results': results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ttm_planner(train_data, test_data, context_length=512, forecast_length=96):\n",
    "    # Prepare data\n",
    "    dset_train, dset_valid, dset_test = prepare_ttm_data(\n",
    "        train_data, \n",
    "        context_length, \n",
    "        forecast_length\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_ttm_planner(context_length, forecast_length)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"ttm_planning_model\",\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\"\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_valid,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_test_set(trainer.model, test_data)\n",
    "    return trainer.model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(json_file, test_size=0.2, random_state=42):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_data, test_data = train_test_split(\n",
    "        data, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data):\n",
    "    # Convert each plan into sequences\n",
    "    X = []  # Input sequences\n",
    "    y = []  # Next states\n",
    "    \n",
    "    for plan_data in data:\n",
    "        plan = plan_data['plan']\n",
    "        goal = plan_data['goal_state']\n",
    "        \n",
    "        # For each state except the last one\n",
    "        for i in range(len(plan)-1):\n",
    "            # Current state + goal state as input\n",
    "            current = list(plan[i].values())\n",
    "            goal_state = list(goal.values())\n",
    "            X.append(current + goal_state)  # Concatenate current and goal\n",
    "            \n",
    "            # Next state as target\n",
    "            next_state = list(plan[i+1].values())\n",
    "            y.append(next_state)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40\n",
      "Test set size: 10\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_and_split_data('dataset.json')\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_sequences(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9114\n",
      "Epoch 10, Loss: 1.5399\n",
      "Epoch 20, Loss: 0.6199\n",
      "Epoch 30, Loss: 0.3702\n",
      "Epoch 40, Loss: 0.3317\n",
      "Epoch 50, Loss: 0.2879\n",
      "Epoch 60, Loss: 0.2234\n",
      "Epoch 70, Loss: 0.1398\n",
      "Epoch 80, Loss: 0.0740\n",
      "Epoch 90, Loss: 0.0474\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_test_set(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation Results:\n",
      "Total cases: 10\n",
      "Success Rate: 30.00%\n",
      "Average Steps: inf\n",
      "Average Valid States Ratio: 66.36%\n",
      "Valid Rate: 30.00%\n",
      "Average Reference Length: 2.40\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Total cases: {len(test_data)}\")\n",
    "print(f\"Success Rate: {test_results['success_rate']:.2%}\")\n",
    "print(f\"Average Steps: {test_results['avg_steps_to_goal']:.2f}\")\n",
    "print(f\"Average Valid States Ratio: {test_results['avg_valid_states_ratio']:.2%}\")\n",
    "print(f\"Valid Rate: {test_results['valid_rate']:.2%}\")\n",
    "print(f\"Average Reference Length: {test_results['avg_reference_length']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of failed cases: 7\n",
      "Sample failed case metrics:\n",
      "{'completely_valid': False,\n",
      " 'goal_reached': False,\n",
      " 'reference_length': 3,\n",
      " 'steps_to_goal': inf,\n",
      " 'valid_states_ratio': 0.5454545454545454}\n"
     ]
    }
   ],
   "source": [
    "failed_cases = [r for r in test_results['detailed_results'] if not r['goal_reached']]\n",
    "if failed_cases:\n",
    "    print(f\"\\nNumber of failed cases: {len(failed_cases)}\")\n",
    "    print(\"Sample failed case metrics:\")\n",
    "    pprint(failed_cases[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsplans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
