{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlocksWorld TTM\n",
    "Use the **2nd Encoding** format to use the TTM Granite model on the BlocksWorld domain.\n",
    "\n",
    "Key modifications from standard TTM:\n",
    "- Input format includes goal state concatenated with current state\n",
    "- Binary state prediction instead of continuous values\n",
    "- Custom metrics for planning success\n",
    "- Sequence padding to handle variable-length plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "from tsfm_public import TimeSeriesPreprocessor, TrackingCallback\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "\n",
    "from BlocksWorld import BlocksWorldGenerator\n",
    "from BlocksWorldValidator import ValidationResult, BlocksWorldValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 13\n",
    "set_seed(SEED)\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Determine device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan Dataclass\n",
    "For storing individual planning examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    context_length: int = 512\n",
    "    prediction_length: int = 96\n",
    "    learning_rate: float = 1e-4\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 50\n",
    "    state_dim: Optional[int] = None  # Will be set during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BlocksWorldSample:\n",
    "    initial_state: List[int]\n",
    "    goal_state: List[int]\n",
    "    plan: List[List[int]]\n",
    "    actions: List[List[str]]\n",
    "    feature_names: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom BlocksWorld Dataset Class\n",
    "The class handles:\n",
    "  - Loading JSON plan data\n",
    "  - Padding sequences to match context length\n",
    "  - Combining state and goal information\n",
    "  - Converting to appropriate tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlocksWorldDataset(Dataset):\n",
    "    def __init__(self, data_path: str, context_length: int, prediction_length: int):\n",
    "        self.context_length: int = context_length\n",
    "        self.prediction_length: int = prediction_length\n",
    "        self.device = DEVICE\n",
    "\n",
    "        with open(data_path, 'r') as f:\n",
    "            raw_data = json.load(f)['plans']\n",
    "\n",
    "        self.samples: List[BlocksWorldSample] = []\n",
    "        for item in raw_data:\n",
    "            sample = BlocksWorldSample(\n",
    "                initial_state=item['initial_state'],\n",
    "                goal_state=item['goal_state'],\n",
    "                plan=item['plan'],\n",
    "                actions=item['actions'],\n",
    "                feature_names=item['feature_names']\n",
    "            )\n",
    "            self.samples.append(sample)\n",
    "\n",
    "        # Get dimensionality from first sample\n",
    "        self.state_dim: int = len(self.samples[0].initial_state)\n",
    "\n",
    "    def __len__(self):  # Length of the Dataset\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Pad plan sequence to match context_length + prediction_length\n",
    "        plan_len = len(sample.plan)\n",
    "        full_seq = sample.plan + [sample.goal_state] * (self.context_length + self.prediction_length - plan_len)\n",
    "        \n",
    "        # Split into past and future\n",
    "        past_seq = full_seq[:self.context_length]\n",
    "        future_seq = full_seq[self.context_length:self.context_length + self.prediction_length]\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        past_values = np.array(past_seq, dtype=np.float32)\n",
    "        future_values = np.array(future_seq, dtype=np.float32)\n",
    "        \n",
    "        # Create masks (1 indicates valid values)\n",
    "        past_observed_mask = np.ones((self.context_length, self.state_dim), dtype=np.float32)\n",
    "        future_observed_mask = np.ones((self.prediction_length, self.state_dim), dtype=np.float32)\n",
    "        \n",
    "        # Include goal state as static categorical feature\n",
    "        static_categorical_values = np.array(sample.goal_state, dtype=np.float32)\n",
    "\n",
    "        return {\n",
    "            \"past_values\": torch.tensor(past_values, dtype=torch.float32).to(self.device),\n",
    "            \"future_values\": torch.tensor(future_values, dtype=torch.float32).to(self.device),\n",
    "            \"past_observed_mask\": torch.tensor(past_observed_mask, dtype=torch.float32).to(self.device),\n",
    "            \"future_observed_mask\": torch.tensor(future_observed_mask, dtype=torch.float32).to(self.device),\n",
    "            \"static_categorical_values\": torch.tensor(static_categorical_values, dtype=torch.float32).to(self.device),\n",
    "            \"freq_token\": torch.zeros(1, dtype=torch.long).to(self.device)  # Placeholder for TTM\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why we need padding?**\n",
    "The model expects every input sequence to be exactly `context_length` timesteps long, but our planning sequences can vary in length (some plans take 3 steps, others might take 10). So, the padding strategy is, (1) when plan is too short, pad with the goal state or (2) when plan is too long, truncate to context_length.\n",
    "\n",
    "For example, if we have:\n",
    "```python\n",
    "context_length = 5\n",
    "plan = [[1,0,0], [1,1,0], [0,1,1]]  # 3 steps\n",
    "goal_state = [0,1,1]\n",
    "```\n",
    "\n",
    "After padding:\n",
    "```python\n",
    "padded_plan = [\n",
    "    [1,0,0],    # Original step 1\n",
    "    [1,1,0],    # Original step 2\n",
    "    [0,1,1],    # Original step 3\n",
    "    [0,1,1],    # Padded with goal\n",
    "    [0,1,1]     # Padded with goal\n",
    "]\n",
    "```\n",
    "\n",
    "Why pad with goal state instead of zeros?:\n",
    "1. **Semantic Meaning**: Using the goal state maintains the logical meaning - \"after reaching the goal, we stay in the goal state\"\n",
    "2. **Learning Signal**: It helps the model understand that reaching the goal is a stable state\n",
    "3. **Consistency**: Ensures all states in the sequence are valid block configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlocksWorld-Based TTM Class\n",
    "To handle training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlocksWorldTTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_length: int = 512,\n",
    "        prediction_length: int = 96,\n",
    "        learning_rate: float = 1e-4,\n",
    "        batch_size: int = 32,\n",
    "        num_epochs: int = 50\n",
    "    ):\n",
    "        self.config = ModelConfig(\n",
    "            context_length=context_length,\n",
    "            prediction_length=prediction_length,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "        self.device = DEVICE\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def train(self, train_dataset: Dataset, val_dataset: Optional[Dataset] = None):\n",
    "        \"\"\"Train the model on given datasets\"\"\"\n",
    "        # Store state dimension from training data\n",
    "        self.config.state_dim = train_dataset.dataset.state_dim if hasattr(train_dataset, 'dataset') else train_dataset.state_dim\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=self.config.context_length,\n",
    "            prediction_length=self.config.prediction_length,\n",
    "            head_dropout=0.1\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"blocks_world_ttm\",\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            evaluation_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            TrackingCallback(),\n",
    "            EarlyStoppingCallback(early_stopping_patience=5)\n",
    "        ]\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.config.learning_rate,\n",
    "            epochs=self.config.num_epochs,\n",
    "            steps_per_epoch=math.ceil(len(train_dataset) / self.config.batch_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=(optimizer, scheduler)\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict(self, initial_states: torch.Tensor, goal_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate action sequences to reach goals from given states\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model needs to be trained or loaded before prediction\")\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_size = initial_states.shape[0]\n",
    "            \n",
    "            # Create context sequence by repeating initial states\n",
    "            context_sequence = initial_states.unsqueeze(1).repeat(1, self.config.context_length, 1)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(self.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(self.device),\n",
    "                \"static_categorical_values\": goal_states.to(self.device),\n",
    "                \"freq_token\": torch.zeros(batch_size, dtype=torch.long).to(self.device)\n",
    "            }\n",
    "            \n",
    "            # Generate predictions\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.sigmoid(outputs[0])\n",
    "            predictions = torch.round(predictions)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model weights and configuration\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"No model to save. Train or load a model first.\")\n",
    "            \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        # Save model state\n",
    "        model_path = os.path.join(path, \"model.pt\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        \n",
    "        # Save configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(asdict(self.config), f)\n",
    "            \n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'BlocksWorldTTM':\n",
    "        \"\"\"Load model weights and configuration\"\"\"\n",
    "        # Load configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "            \n",
    "        # Create instance with loaded config\n",
    "        instance = cls(\n",
    "            context_length=config_dict['context_length'],\n",
    "            prediction_length=config_dict['prediction_length'],\n",
    "            learning_rate=config_dict['learning_rate'],\n",
    "            batch_size=config_dict['batch_size'],\n",
    "            num_epochs=config_dict['num_epochs']\n",
    "        )\n",
    "        instance.config.state_dim = config_dict['state_dim']\n",
    "        \n",
    "        # Initialize and load model\n",
    "        instance.model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=instance.config.context_length,\n",
    "            prediction_length=instance.config.prediction_length,\n",
    "            head_dropout=0.1\n",
    "        ).to(instance.device)\n",
    "        \n",
    "        model_path = os.path.join(path, \"model.pt\")\n",
    "        instance.model.load_state_dict(torch.load(model_path, map_location=instance.device))\n",
    "        instance.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model receives these key components for each sample during **training**:\n",
    "1. Past Values (past_values):\n",
    "    \n",
    "    `past_values = torch.tensor(past_seq, dtype=torch.float32)`\n",
    "    - Shape: [batch_size, context_length, state_dim]\n",
    "    - These are sequences of states leading up to the current point\n",
    "    - Each state is a binary vector representing the blocks world predicates\n",
    "    - Length is padded to context_length (512 by default)\n",
    "2. Future Values (future_values):\n",
    "    \n",
    "    `future_values = torch.tensor(future_seq, dtype=torch.float32)`\n",
    "    - Shape: [batch_size, prediction_length, state_dim]`\n",
    "    - These are the target sequences of states we want to predict\n",
    "    - Length is padded to prediction_length (96 by default)\n",
    "3. Observation Masks:\n",
    "    \n",
    "    `past_observed_mask = torch.ones((context_length, state_dim))`\n",
    "\n",
    "    `future_observed_mask = torch.ones((prediction_length, state_dim))`\n",
    "    - Binary masks indicating which values are valid (1) vs padding (0)\n",
    "    - Helps model ignore padded values during training\n",
    "4. Static Categorical Values:\n",
    "    \n",
    "    `static_categorical_values = torch.tensor(sample.goal_state)`\n",
    "    - Shape: [batch_size, state_dim]\n",
    "    - The goal state we want to reach\n",
    "    - Stays constant across the entire sequence\n",
    "    - Helps guide the prediction towards the goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Prediction the model takes:\n",
    "1. Initial States:\n",
    "    \n",
    "    `past_values = torch.tensor(initial_states).unsqueeze(1).repeat(1, context_length, 1)`\n",
    "    - The starting state is repeated to fill the context window\n",
    "    - This gives the model the current state as context\n",
    "2. Goal States:\n",
    "\n",
    "    `static_categorical_values = torch.tensor(goal_states)`\n",
    "    - Target goal state as static features\n",
    "    - Guides the generation of the plan\n",
    "\n",
    "The model outputs:\n",
    "\n",
    "```\n",
    "predictions = torch.sigmoid(outputs[0])  # Convert to probabilities\n",
    "predictions = torch.round(predictions)   # Convert to binary states\n",
    "```\n",
    "\n",
    "- Shape: [batch_size, prediction_length, state_dim]\n",
    "- Sequence of predicted states forming a plan\n",
    "- Each state is a binary vector matching the input encoding\n",
    "- The sequence should transition from initial state to goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data_path: str, context_length: int, prediction_length: int):\n",
    "    \"\"\"Create train/val/test datasets\"\"\"\n",
    "    full_dataset = BlocksWorldDataset(data_path, context_length, prediction_length)\n",
    "    \n",
    "    # Split indices\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.15 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, targets):\n",
    "    \"\"\"Compute metrics for predicted plans\"\"\"\n",
    "    predictions = predictions.numpy()\n",
    "    targets = targets.numpy()\n",
    "    \n",
    "    # State prediction accuracy\n",
    "    state_accuracy = np.mean(predictions == targets)\n",
    "    \n",
    "    # Goal achievement rate (exact match of final state)\n",
    "    goal_achieved = np.all(predictions[:, -1] == targets[:, -1], axis=1)\n",
    "    goal_achievement_rate = np.mean(goal_achieved)\n",
    "    \n",
    "    # Partial goal achievement (percentage of correct final state bits)\n",
    "    partial_goal = np.mean(predictions[:, -1] == targets[:, -1], axis=1)\n",
    "    avg_partial_goal = np.mean(partial_goal)\n",
    "    \n",
    "    return {\n",
    "        \"state_accuracy\": state_accuracy,\n",
    "        \"goal_achievement_rate\": goal_achievement_rate,\n",
    "        \"avg_partial_goal\": avg_partial_goal\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the model focusing on goal state prediction\n",
    "    \"\"\"\n",
    "    model.model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    goal_state_predictions = []\n",
    "    goal_state_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            \n",
    "            # Get initial and goal states\n",
    "            initial_state = sample['past_values'][0]  # First timestep\n",
    "            goal_state = sample['static_categorical_values']\n",
    "            target = sample['future_values']\n",
    "            \n",
    "            # Create context sequence by repeating initial state\n",
    "            context_sequence = initial_state.unsqueeze(0).repeat(1, model.config.context_length, 1)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(model.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(model.device),\n",
    "                \"static_categorical_values\": goal_state.unsqueeze(0).to(model.device),\n",
    "                \"freq_token\": torch.zeros(1, dtype=torch.long).to(model.device)\n",
    "            }\n",
    "            \n",
    "            # Get model prediction\n",
    "            outputs = model.model(**inputs)\n",
    "            prediction = torch.sigmoid(outputs[0])\n",
    "            prediction = torch.round(prediction)\n",
    "            \n",
    "            # Only keep the relevant part of the prediction \n",
    "            # (same length as target and same feature dimension)\n",
    "            prediction = prediction[:, :target.shape[0], :target.shape[-1]]\n",
    "            \n",
    "            all_predictions.append(prediction)\n",
    "            all_targets.append(target)\n",
    "            \n",
    "            # Store just the final states (goal states)\n",
    "            goal_state_predictions.append(prediction[:, -1])\n",
    "            goal_state_targets.append(target[-1])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.stack(all_targets, dim=0)\n",
    "    goal_state_predictions = torch.cat(goal_state_predictions, dim=0)\n",
    "    goal_state_targets = torch.stack(goal_state_targets, dim=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        # Overall sequence metrics\n",
    "        \"sequence_accuracy\": torch.mean((all_predictions == all_targets).float()).item(),\n",
    "        \"sequence_hamming_distance\": torch.mean(torch.sum((all_predictions != all_targets).float(), dim=(1,2))).item(),\n",
    "        \n",
    "        # Goal state specific metrics\n",
    "        \"goal_state_accuracy\": torch.mean((goal_state_predictions == goal_state_targets).float()).item(),\n",
    "        \"goal_state_hamming_distance\": torch.mean(torch.sum((goal_state_predictions != goal_state_targets).float(), dim=-1)).item(),\n",
    "        \n",
    "        # Partial goal achievement (percentage of correct bits in goal state)\n",
    "        \"partial_goal_achievement\": torch.mean(torch.mean((goal_state_predictions == goal_state_targets).float(), dim=-1)).item(),\n",
    "        \n",
    "        # Perfect goal achievement rate (exact matches)\n",
    "        \"perfect_goal_achievement_rate\": torch.mean(torch.all(goal_state_predictions == goal_state_targets, dim=-1).float()).item(),\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nModel Evaluation Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        for name, value in metrics.items():\n",
    "            print(f\"{name}: {value:.4f}\")\n",
    "            \n",
    "        # Print shapes for debugging\n",
    "        print(\"\\nTensor shapes:\")\n",
    "        print(f\"Predictions shape: {all_predictions.shape}\")\n",
    "        print(f\"Targets shape: {all_targets.shape}\")\n",
    "        print(f\"Goal state predictions shape: {goal_state_predictions.shape}\")\n",
    "        print(f\"Goal state targets shape: {goal_state_targets.shape}\")\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_patterns(model, test_dataset, n_samples=5):\n",
    "    \"\"\"\n",
    "    Analyze specific cases where the model fails or succeeds\n",
    "    \"\"\"\n",
    "    model.model.eval()\n",
    "    successes = []\n",
    "    failures = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            \n",
    "            # Get initial and goal states\n",
    "            initial_state = sample['past_values'][0]\n",
    "            goal_state = sample['static_categorical_values']\n",
    "            target = sample['future_values'][-1]\n",
    "            \n",
    "            # Create context sequence\n",
    "            context_sequence = initial_state.unsqueeze(0).repeat(1, model.config.context_length, 1)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(model.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(model.device),\n",
    "                \"static_categorical_values\": goal_state.unsqueeze(0).to(model.device),\n",
    "                \"freq_token\": torch.zeros(1, dtype=torch.long).to(model.device)\n",
    "            }\n",
    "            \n",
    "            # Get prediction\n",
    "            outputs = model.model(**inputs)\n",
    "            prediction = torch.sigmoid(outputs[0])\n",
    "            prediction = torch.round(prediction)\n",
    "            predicted_goal = prediction[0, -1]\n",
    "            \n",
    "            # Check if prediction matches target\n",
    "            is_correct = torch.all(predicted_goal == target)\n",
    "            \n",
    "            case = {\n",
    "                'initial_state': initial_state.cpu().numpy(),\n",
    "                'goal_state': goal_state.cpu().numpy(),\n",
    "                'predicted_goal': predicted_goal.cpu().numpy(),\n",
    "                'target_goal': target.cpu().numpy(),\n",
    "                'hamming_distance': torch.sum((predicted_goal != target).float()).item()\n",
    "            }\n",
    "            \n",
    "            if is_correct:\n",
    "                successes.append(case)\n",
    "            else:\n",
    "                failures.append(case)\n",
    "            \n",
    "            if len(successes) >= n_samples and len(failures) >= n_samples:\n",
    "                break\n",
    "    \n",
    "    return successes[:n_samples], failures[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks in the dataset: 3\n",
      "Train size: 35, Val size: 7, Test size: 8\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "dataset_file = \"../data/dataset_3.json\"\n",
    "print(f\"Number of blocks in the dataset: {(num_blocks := int(dataset_file.split(\"_\")[-1][0]))}\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "    dataset_file,\n",
    "    context_length=512,\n",
    "    prediction_length=96\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and train model\n",
    "# ttm = BlocksWorldTTM(\n",
    "#     context_length=512,\n",
    "#     prediction_length=96,\n",
    "#     learning_rate=1e-4,\n",
    "#     batch_size=32,\n",
    "#     num_epochs=50\n",
    "# )\n",
    "\n",
    "# print(\"Starting training...\")\n",
    "# ttm.train(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & Load Model\n",
    "save_path = f\"../models/blocks_world_ttm_{num_blocks}\"\n",
    "\n",
    "# ttm.save(save_path)\n",
    "# print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-56849:t-8250786368:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "INFO:p-56849:t-8250786368:get_model.py:get_model:Selected prediction_length = 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-56849:t-8250786368:get_model.py:get_model:Model loaded successfully!\n",
      "INFO:p-56849:t-8250786368:get_model.py:get_model:[TTM] context_len = 512, forecast_len = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/blocks_world_ttm_3\n",
      "Loaded from ../models/blocks_world_ttm_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/q_twsl9n5lg8f5977cn3v4jh0000gn/T/ipykernel_56849/2531237081.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  instance.model.load_state_dict(torch.load(model_path, map_location=instance.device))\n"
     ]
    }
   ],
   "source": [
    "ttm = BlocksWorldTTM.load(save_path)\n",
    "print(f\"Loaded from {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "--------------------------------------------------\n",
      "sequence_accuracy: 0.7413\n",
      "sequence_hamming_distance: 372.5000\n",
      "goal_state_accuracy: 0.7500\n",
      "goal_state_hamming_distance: 3.7500\n",
      "partial_goal_achievement: 0.7500\n",
      "perfect_goal_achievement_rate: 0.1250\n",
      "\n",
      "Tensor shapes:\n",
      "Predictions shape: torch.Size([8, 96, 15])\n",
      "Targets shape: torch.Size([8, 96, 15])\n",
      "Goal state predictions shape: torch.Size([8, 15])\n",
      "Goal state targets shape: torch.Size([8, 15])\n",
      "\n",
      "Analyzing error patterns...\n",
      "\n",
      "--------------------------------------------------\n",
      "Length of the test dataset: 8\n",
      "\n",
      "Example Successes:\n",
      "\n",
      "Case 1:\n",
      "Initial State: BlockState(clear={'B', 'A'}, on_table={'C', 'A'}, on={'B': 'C'}, holding=None)\n",
      "Goal State: BlockState(clear={'B', 'A'}, on_table={'C', 'A'}, on={'B': 'C'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'B', 'A'}, on_table={'C', 'A'}, on={'B': 'C'}, holding=None)\n",
      "\n",
      "Example Failures:\n",
      "\n",
      "Case 1:\n",
      "Initial State: BlockState(clear={'C', 'B'}, on_table={'C', 'A'}, on={'B': 'A'}, holding=None)\n",
      "Goal State: BlockState(clear={'C', 'B', 'A'}, on_table={'C', 'B', 'A'}, on={}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'C', 'B'}, on_table={'C', 'A'}, on={'B': 'A'}, holding=None)\n",
      "Target Goal: BlockState(clear={'C', 'B', 'A'}, on_table={'C', 'B', 'A'}, on={}, holding=None)\n",
      "Hamming Distance: 3.0\n",
      "\n",
      "Case 2:\n",
      "Initial State: BlockState(clear={'C', 'B'}, on_table={'B', 'A'}, on={'C': 'A'}, holding=None)\n",
      "Goal State: BlockState(clear={'C', 'A'}, on_table={'B', 'A'}, on={'C': 'B'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'C', 'B'}, on_table={'B', 'A'}, on={'C': 'A'}, holding=None)\n",
      "Target Goal: BlockState(clear={'C', 'A'}, on_table={'B', 'A'}, on={'C': 'B'}, holding=None)\n",
      "Hamming Distance: 4.0\n",
      "\n",
      "Case 3:\n",
      "Initial State: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'C', 'C': 'A'}, holding=None)\n",
      "Goal State: BlockState(clear={'B', 'A'}, on_table={'C', 'A'}, on={'B': 'C'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'C', 'C': 'A'}, holding=None)\n",
      "Target Goal: BlockState(clear={'B', 'A'}, on_table={'C', 'A'}, on={'B': 'C'}, holding=None)\n",
      "Hamming Distance: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "metrics = evaluate_model(ttm, test_dataset)\n",
    "\n",
    "# Analyze error patterns\n",
    "print(\"\\nAnalyzing error patterns...\")\n",
    "successes, failures = analyze_error_patterns(ttm, test_dataset)\n",
    "\n",
    "# Examine the inital states, goal states, and actions\n",
    "gen = BlocksWorldGenerator(num_blocks=num_blocks)\n",
    "\n",
    "print(\"\\n--------------------------------------------------\")\n",
    "print(f\"Length of the test dataset: {len(test_dataset)}\")\n",
    "print(\"\\nExample Successes:\")\n",
    "for i, case in enumerate(successes[:3]):\n",
    "    print(f\"\\nCase {i+1}:\")\n",
    "    print(f\"Initial State: {gen.decode_vector_to_blocks(case['initial_state'])}\")\n",
    "    print(f\"Goal State: {gen.decode_vector_to_blocks(case['goal_state'])}\")\n",
    "    print(f\"Predicted Goal: {gen.decode_vector_to_blocks(case['predicted_goal'])}\")\n",
    "    \n",
    "print(\"\\nExample Failures:\")\n",
    "for i, case in enumerate(failures[:3]):\n",
    "    print(f\"\\nCase {i+1}:\")\n",
    "    print(f\"Initial State: {gen.decode_vector_to_blocks(case['initial_state'])}\")\n",
    "    print(f\"Goal State: {gen.decode_vector_to_blocks(case['goal_state'])}\")\n",
    "    print(f\"Predicted Goal: {gen.decode_vector_to_blocks(case['predicted_goal'])}\")\n",
    "    print(f\"Target Goal: {gen.decode_vector_to_blocks(case['target_goal'])}\")\n",
    "    print(f\"Hamming Distance: {case['hamming_distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of BlocksWorld Metrics\n",
    "\n",
    "1. **Sequence Accuracy** (`sequence_accuracy`):\n",
    "\n",
    "```python\n",
    "\"sequence_accuracy\": torch.mean((all_predictions == all_targets).float()).item()\n",
    "```\n",
    "- Measures how accurately the model predicts the entire sequence of states\n",
    "- Compares each predicted state with the target state at each timestep\n",
    "- Value ranges from 0 to 1, where 1 means perfect prediction of all states in the sequence\n",
    "\n",
    "2. **Sequence Hamming Distance** (`sequence_hamming_distance`):\n",
    "\n",
    "```python\n",
    "\"sequence_hamming_distance\": torch.mean(torch.sum((all_predictions != all_targets).float(), dim=-1)).item()\n",
    "```\n",
    "- Measures the average number of bits that differ between predicted and target sequences\n",
    "- Higher values indicate more differences between predicted and actual states\n",
    "- Useful for understanding how \"far off\" predictions are from targets\n",
    "\n",
    "3. **Goal State Accuracy** (`goal_state_accuracy`):\n",
    "\n",
    "```python\n",
    "\"goal_state_accuracy\": torch.mean((goal_state_predictions == goal_state_targets).float()).item()\n",
    "```\n",
    "- Measures accuracy of just the final state prediction (ignoring intermediate states)\n",
    "- Focuses on whether the model reaches the correct goal state\n",
    "- Value between 0 and 1, where 1 means perfect goal state prediction\n",
    "\n",
    "4. **Goal State Hamming Distance** (`goal_state_hamming_distance`):\n",
    "\n",
    "```python\n",
    "\"goal_state_hamming_distance\": torch.mean(torch.sum((goal_state_predictions != goal_state_targets).float(), dim=-1)).item()\n",
    "```\n",
    "- Number of bits that differ between predicted and target goal states\n",
    "- Lower values are better\n",
    "- Helps understand how close the predicted goal state is to the target\n",
    "\n",
    "5. **Partial Goal Achievement** (`partial_goal_achievement`):\n",
    "\n",
    "```python\n",
    "\"partial_goal_achievement\": torch.mean(torch.mean((goal_state_predictions == goal_state_targets).float(), dim=-1)).item()\n",
    "```\n",
    "- Percentage of correctly predicted bits in the goal state\n",
    "- Useful for understanding partial success\n",
    "- Example: If 8 out of 10 bits are correct, value would be 0.8\n",
    "\n",
    "6. **Perfect Goal Achievement Rate** (`perfect_goal_achievement_rate`):\n",
    "\n",
    "```python\n",
    "\"perfect_goal_achievement_rate\": torch.mean(torch.all(goal_state_predictions == goal_state_targets, dim=-1).float()).item()\n",
    "```\n",
    "- Proportion of cases where the goal state is perfectly predicted\n",
    "- Stricter than partial goal achievement\n",
    "- Only counts exact matches as successes\n",
    "\n",
    "These metrics help understand:\n",
    "- Overall sequence prediction quality (sequence_accuracy, sequence_hamming_distance)\n",
    "- Goal achievement quality (goal_state_accuracy, goal_state_hamming_distance)\n",
    "- Partial vs Perfect success rates (partial_goal_achievement, perfect_goal_achievement_rate)\n",
    "\n",
    "For the blocks world domain:\n",
    "- A high sequence accuracy means the model predicts valid intermediate states\n",
    "- A low goal state hamming distance means predictions are close to desired configurations\n",
    "- Perfect goal achievement rate shows how often the model reaches exactly the right block configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsplans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
