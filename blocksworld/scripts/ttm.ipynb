{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlocksWorld TTM\n",
    "\n",
    "Use the **2nd Encoding** format to use the TTM Granite model on the BlocksWorld domain.\n",
    "\n",
    "Key modifications from standard TTM:\n",
    "\n",
    "- Input format includes goal state concatenated with current state\n",
    "- Binary state prediction instead of continuous values\n",
    "- Custom metrics for planning success\n",
    "- Sequence padding to handle variable-length plans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from pprint import pformat\n",
    "\n",
    "from tsfm_public import TrackingCallback\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "\n",
    "from BlocksWorld import BlocksWorldGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: [13, 'ibm-granite/granite-timeseries-ttm-r2', device(type='mps')]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "SEED = 13\n",
    "set_seed(SEED)\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "\n",
    "# Determine device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using: {pformat([SEED, TTM_MODEL_PATH, DEVICE])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan Dataclass\n",
    "\n",
    "For storing individual planning examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    context_length: int = 1  # Placeholder for context length\n",
    "    prediction_length: int = 96\n",
    "    learning_rate: float = 1e-4\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 50\n",
    "    state_dim: Optional[int] = None  # Will be set during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BlocksWorldSample:\n",
    "    initial_state: List[int]\n",
    "    goal_state: List[int]\n",
    "    plan: List[List[int]]\n",
    "    actions: List[List[str]]\n",
    "    feature_names: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom BlocksWorld Dataset Class\n",
    "\n",
    "The class handles:\n",
    "\n",
    "- Loading JSON plan data\n",
    "- Padding sequences to match context length\n",
    "- Combining state and goal information\n",
    "- Converting to appropriate tensor format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlocksWorldDataset(Dataset):\n",
    "    def __init__(self, data_path: str, context_length: int, prediction_length: int):\n",
    "        self.context_length: int = context_length\n",
    "        self.prediction_length: int = prediction_length\n",
    "        self.device = DEVICE\n",
    "\n",
    "        with open(data_path, \"r\") as f:\n",
    "            raw_data = json.load(f)[\"plans\"]\n",
    "\n",
    "        self.samples: List[BlocksWorldSample] = []\n",
    "        for item in raw_data:\n",
    "            sample = BlocksWorldSample(\n",
    "                initial_state=item[\"initial_state\"],\n",
    "                goal_state=item[\"goal_state\"],\n",
    "                plan=item[\"plan\"],\n",
    "                actions=item[\"actions\"],\n",
    "                feature_names=item[\"feature_names\"],\n",
    "            )\n",
    "            self.samples.append(sample)\n",
    "\n",
    "        # Get dimensionality from first sample\n",
    "        self.state_dim: int = len(self.samples[0].initial_state)\n",
    "\n",
    "    def __len__(self):  # Length of the Dataset\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        plan_states_np = np.array(sample.plan, dtype=np.float32)  # shape (plan_len, state_dim)\n",
    "        goal_state_np = np.array(sample.goal_state, dtype=np.float32)  # shape (state_dim,)\n",
    "        plan_len = len(sample.plan)\n",
    "\n",
    "        # Past values and mask\n",
    "        past_values_np = np.zeros((self.context_length, self.state_dim), dtype=np.float32)\n",
    "        past_observed_mask_np = np.zeros((self.context_length, self.state_dim), dtype=np.float32)\n",
    "\n",
    "        # How many actual plan steps can go into context\n",
    "        len_from_plan_for_past = min(plan_len, self.context_length)\n",
    "        past_values_np[:len_from_plan_for_past] = plan_states_np[:len_from_plan_for_past]\n",
    "        past_observed_mask_np[:len_from_plan_for_past, :] = 1.0\n",
    "\n",
    "        # Fill remaining past_values with padding (e.g., last valid state or goal state)\n",
    "        # If using goal_state for padding context...\n",
    "        if len_from_plan_for_past < self.context_length:\n",
    "            num_past_padding = self.context_length - len_from_plan_for_past\n",
    "            padding_values = np.tile(goal_state_np, (num_past_padding, 1))\n",
    "            past_values_np[len_from_plan_for_past:] = padding_values\n",
    "            # Mask for these padded values remains 0\n",
    "\n",
    "        # Future values and mask\n",
    "        future_values_np = np.zeros((self.prediction_length, self.state_dim), dtype=np.float32)\n",
    "        future_observed_mask_np = np.zeros((self.prediction_length, self.state_dim), dtype=np.float32)\n",
    "\n",
    "        # How many actual plan steps can go into future (after context)\n",
    "        # These are sample.plan[self.context_length:]\n",
    "        actual_future_steps_from_plan = plan_states_np[self.context_length :]\n",
    "        len_from_plan_for_future = min(len(actual_future_steps_from_plan), self.prediction_length)\n",
    "\n",
    "        if len_from_plan_for_future > 0:\n",
    "            future_values_np[:len_from_plan_for_future] = actual_future_steps_from_plan[:len_from_plan_for_future]\n",
    "            future_observed_mask_np[:len_from_plan_for_future, :] = 1.0\n",
    "\n",
    "        # Fill remaining future_values with goal_state padding\n",
    "        if len_from_plan_for_future < self.prediction_length:\n",
    "            num_future_padding = self.prediction_length - len_from_plan_for_future\n",
    "            padding_values = np.tile(goal_state_np, (num_future_padding, 1))\n",
    "            future_values_np[len_from_plan_for_future:] = padding_values\n",
    "            # Mask for these padded values remains 0 (model learns to predict goal, but loss ignores if it's just padding)\n",
    "            # OR, to make the model  explicitly learn to output goal for padding, mask should be 1.\n",
    "            # The TTM examples usually mask out padding in targets.\n",
    "            # This is a design choice. If mask=1, loss is computed. If mask=0, loss is ignored.\n",
    "            # For learning stability at goal, mask=1 for goal padding in future_values could be reasonable.\n",
    "            future_observed_mask_np[len_from_plan_for_future:, :] = (\n",
    "                # 1.0  # To enforce goal prediction\n",
    "                0.0  # To ignore loss on goal prediction\n",
    "            )\n",
    "\n",
    "        # Static categorical values (goal state)\n",
    "        static_categorical_values_np = goal_state_np\n",
    "\n",
    "        return {\n",
    "            \"past_values\": torch.tensor(past_values_np, dtype=torch.float32).to(self.device),\n",
    "            \"future_values\": torch.tensor(future_values_np, dtype=torch.float32).to(self.device),\n",
    "            \"past_observed_mask\": torch.tensor(past_observed_mask_np, dtype=torch.float32).to(self.device),\n",
    "            \"future_observed_mask\": torch.tensor(future_observed_mask_np, dtype=torch.float32).to(self.device),\n",
    "            \"static_categorical_values\": torch.tensor(static_categorical_values_np, dtype=torch.float32).to(\n",
    "                self.device\n",
    "            ),\n",
    "            \"freq_token\": torch.zeros(1, dtype=torch.long).to(self.device),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlocksWorld-Based TTM Class\n",
    "\n",
    "To handle training and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlocksWorldTTM:\n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        self.config = model_config\n",
    "        self.device = DEVICE\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.model_name = None\n",
    "\n",
    "    def train(self, train_dataset: Dataset, val_dataset: Optional[Dataset] = None):\n",
    "        \"\"\"Train the model on given datasets\"\"\"\n",
    "        # Store state dimension from training data\n",
    "        self.config.state_dim = (\n",
    "            train_dataset.dataset.state_dim if hasattr(train_dataset, \"dataset\") else train_dataset.state_dim\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=self.config.context_length,\n",
    "            prediction_length=self.config.prediction_length,\n",
    "            head_dropout=0.1,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.model_name = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=self.config.context_length,\n",
    "            prediction_length=self.config.prediction_length,\n",
    "            head_dropout=0.1,\n",
    "            return_model_key=True,\n",
    "        )\n",
    "        print(f\"Received model name: {self.model_name}\")\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"blocks_world_ttm\",\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            eval_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\",\n",
    "            dataloader_pin_memory=False,\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            TrackingCallback(),\n",
    "            EarlyStoppingCallback(early_stopping_patience=5),\n",
    "        ]\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.config.learning_rate,\n",
    "            epochs=self.config.num_epochs,\n",
    "            steps_per_epoch=math.ceil(len(train_dataset) / self.config.batch_size),\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict(self, initial_states: torch.Tensor, goal_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate action sequences to reach goals from given states\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model needs to be trained or loaded before prediction\")\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_size = initial_states.shape[0]\n",
    "\n",
    "            # Create context sequence by repeating initial states\n",
    "            context_sequence = initial_states.unsqueeze(1).repeat(1, self.config.context_length, 1)\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(self.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(self.device),\n",
    "                \"static_categorical_values\": goal_states.to(self.device),\n",
    "                \"freq_token\": torch.zeros(batch_size, dtype=torch.long).to(self.device),\n",
    "            }\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.sigmoid(outputs[0])\n",
    "            predictions = torch.round(predictions)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model weights and configuration\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"No model to save. Train or load a model first.\")\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Save model state\n",
    "        model_path = os.path.join(path, \"model.pt\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        # Save configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(asdict(self.config), f)\n",
    "\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"BlocksWorldTTM\":\n",
    "        \"\"\"Load model weights and configuration\"\"\"\n",
    "        # Load configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_dict = json.load(f)\n",
    "\n",
    "        # Create instance with loaded config\n",
    "        instance = cls(\n",
    "            context_length=config_dict[\"context_length\"],\n",
    "            prediction_length=config_dict[\"prediction_length\"],\n",
    "            learning_rate=config_dict[\"learning_rate\"],\n",
    "            batch_size=config_dict[\"batch_size\"],\n",
    "            num_epochs=config_dict[\"num_epochs\"],\n",
    "        )\n",
    "        instance.config.state_dim = config_dict[\"state_dim\"]\n",
    "\n",
    "        # Initialize and load model\n",
    "        instance.model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=instance.config.context_length,\n",
    "            prediction_length=instance.config.prediction_length,\n",
    "            head_dropout=0.1,\n",
    "        ).to(instance.device)\n",
    "\n",
    "        model_path = os.path.join(path, \"model.pt\")\n",
    "        instance.model.load_state_dict(torch.load(model_path, map_location=instance.device))\n",
    "        instance.model.eval()\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data_path: str, context_length: int, prediction_length: int):\n",
    "    \"\"\"Create train/val/test datasets\"\"\"\n",
    "    full_dataset = BlocksWorldDataset(data_path, context_length, prediction_length)\n",
    "\n",
    "    # Split indices\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.15 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(SEED),\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, verbose=True):\n",
    "    \"\"\"Comprehensive evaluation of the model with more detailed metrics\"\"\"\n",
    "    model.model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    goal_state_predictions = []\n",
    "    goal_state_targets = []\n",
    "\n",
    "    num_samples = len(test_dataset)\n",
    "    num_exact_matches = 0\n",
    "    num_partial_matches = 0\n",
    "    total_bits_correct = 0\n",
    "    total_bits = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            sample = test_dataset[i]\n",
    "\n",
    "            # Get initial and goal states\n",
    "            initial_state = sample[\"past_values\"][0]\n",
    "            goal_state = sample[\"static_categorical_values\"]\n",
    "            target = sample[\"future_values\"]\n",
    "\n",
    "            # Create context sequence\n",
    "            context_sequence = initial_state.unsqueeze(0).repeat(1, model.config.context_length, 1)\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(model.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(model.device),\n",
    "                \"static_categorical_values\": goal_state.unsqueeze(0).to(model.device),\n",
    "                \"freq_token\": torch.zeros(1, dtype=torch.long).to(model.device),\n",
    "            }\n",
    "\n",
    "            # Get prediction\n",
    "            outputs = model.model(**inputs)\n",
    "            prediction = torch.sigmoid(outputs[0])\n",
    "            prediction = torch.round(prediction)\n",
    "\n",
    "            # Store predictions and targets\n",
    "            all_predictions.append(prediction)\n",
    "            all_targets.append(target)\n",
    "\n",
    "            # Focus on goal states (final states)\n",
    "            pred_goal = prediction[0, -1]\n",
    "            true_goal = target[-1]\n",
    "\n",
    "            goal_state_predictions.append(pred_goal)\n",
    "            goal_state_targets.append(true_goal)\n",
    "\n",
    "            # Calculate exact matches\n",
    "            if torch.all(pred_goal == true_goal):\n",
    "                num_exact_matches += 1\n",
    "\n",
    "            # Calculate partial matches (more than 50% bits correct)\n",
    "            num_correct_bits = torch.sum(pred_goal == true_goal).item()\n",
    "            total_bits_correct += num_correct_bits\n",
    "            total_bits += len(pred_goal)\n",
    "\n",
    "            if num_correct_bits > len(pred_goal) / 2:\n",
    "                num_partial_matches += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"num_samples\": num_samples,\n",
    "        \"num_exact_matches\": num_exact_matches,\n",
    "        \"exact_match_rate\": num_exact_matches / num_samples,\n",
    "        \"num_partial_matches\": num_partial_matches,\n",
    "        \"partial_match_rate\": num_partial_matches / num_samples,\n",
    "        \"bit_accuracy\": total_bits_correct / total_bits,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nDetailed Model Evaluation Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total number of test samples: {metrics['num_samples']}\")\n",
    "        print(f\"Number of exact goal state matches: {metrics['num_exact_matches']}\")\n",
    "        print(f\"Exact match rate: {metrics['exact_match_rate']:.4f}\")\n",
    "        print(f\"Number of partial matches (>50% correct): {metrics['num_partial_matches']}\")\n",
    "        print(f\"Partial match rate: {metrics['partial_match_rate']:.4f}\")\n",
    "        print(f\"Bit-level accuracy: {metrics['bit_accuracy']:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def analyze_error_patterns(model, test_dataset, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced error pattern analysis with more detailed statistics\n",
    "    \"\"\"\n",
    "    model.model.eval()\n",
    "    successes = []\n",
    "    failures = []\n",
    "\n",
    "    bit_error_counts = {}  # Track which bits are most commonly wrong\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "\n",
    "            # Get initial and goal states\n",
    "            initial_state = sample[\"past_values\"][0]\n",
    "            goal_state = sample[\"static_categorical_values\"]\n",
    "            target = sample[\"future_values\"][-1]\n",
    "\n",
    "            # Create context sequence\n",
    "            context_sequence = initial_state.unsqueeze(0).repeat(1, model.config.context_length, 1)\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(model.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(model.device),\n",
    "                \"static_categorical_values\": goal_state.unsqueeze(0).to(model.device),\n",
    "                \"freq_token\": torch.zeros(1, dtype=torch.long).to(model.device),\n",
    "            }\n",
    "\n",
    "            # Get prediction\n",
    "            outputs = model.model(**inputs)\n",
    "            prediction = torch.sigmoid(outputs[0])\n",
    "            prediction = torch.round(prediction)\n",
    "            predicted_goal = prediction[0, -1]\n",
    "\n",
    "            # Calculate error statistics\n",
    "            errors = (predicted_goal != target).nonzero().squeeze(1)\n",
    "            num_errors = len(errors)\n",
    "\n",
    "            # Track which bits had errors\n",
    "            for error_idx in errors:\n",
    "                if error_idx.item() not in bit_error_counts:\n",
    "                    bit_error_counts[error_idx.item()] = 0\n",
    "                bit_error_counts[error_idx.item()] += 1\n",
    "\n",
    "            case = {\n",
    "                \"initial_state\": initial_state.cpu().numpy(),\n",
    "                \"goal_state\": goal_state.cpu().numpy(),\n",
    "                \"predicted_goal\": predicted_goal.cpu().numpy(),\n",
    "                \"target_goal\": target.cpu().numpy(),\n",
    "                \"num_errors\": num_errors,\n",
    "                \"error_positions\": errors.cpu().numpy(),\n",
    "            }\n",
    "\n",
    "            if num_errors == 0:\n",
    "                successes.append(case)\n",
    "            else:\n",
    "                failures.append(case)\n",
    "\n",
    "    analysis = {\n",
    "        \"num_successes\": len(successes),\n",
    "        \"num_failures\": len(failures),\n",
    "        \"success_rate\": len(successes) / (len(successes) + len(failures)),\n",
    "        \"bit_error_counts\": bit_error_counts,\n",
    "        \"successes\": successes,\n",
    "        \"failures\": failures,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nError Pattern Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of successful predictions: {analysis['num_successes']}\")\n",
    "        print(f\"Number of failed predictions: {analysis['num_failures']}\")\n",
    "        print(f\"Success rate: {analysis['success_rate']:.4f}\")\n",
    "        print(\"\\nMost common error positions:\")\n",
    "        sorted_errors = sorted(bit_error_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for bit, count in sorted_errors[:5]:\n",
    "            print(f\"Bit {bit}: {count} errors\")\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(data_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze the dataset to determine appropriate parameters\"\"\"\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)[\"plans\"]\n",
    "\n",
    "    # Get key statistics\n",
    "    max_plan_length = max(len(item[\"plan\"]) for item in data)\n",
    "    avg_plan_length = sum(len(item[\"plan\"]) for item in data) / len(data)\n",
    "    state_dim = len(data[0][\"initial_state\"])\n",
    "    num_samples = len(data)\n",
    "\n",
    "    stats = {\n",
    "        \"max_plan_length\": max_plan_length,\n",
    "        \"avg_plan_length\": avg_plan_length,\n",
    "        \"state_dim\": state_dim,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"recommended_prediction_length\": max_plan_length + 2,  # Add small buffer\n",
    "    }\n",
    "\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Maximum plan length: {max_plan_length}\")\n",
    "    print(f\"Average plan length: {avg_plan_length:.2f}\")\n",
    "    print(f\"Recommended prediction length: {stats['recommended_prediction_length']}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks in the dataset: 6\n",
      "\n",
      "Dataset Statistics:\n",
      "Number of samples: 44175\n",
      "State dimension: 48\n",
      "Maximum plan length: 21\n",
      "Average plan length: 9.07\n",
      "Recommended prediction length: 23\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "dataset_file = \"../data/dataset_6.json\"\n",
    "print(f\"Number of blocks in the dataset: {(num_blocks := int(dataset_file.split('_')[-1][0]))}\")\n",
    "\n",
    "# Analyze dataset\n",
    "stats = analyze_dataset(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max plan length: 21 | Selected context length: 52\n",
      "Recommended prediction length: 23 | Selected forecast length: 16\n"
     ]
    }
   ],
   "source": [
    "# For the context length, we need to get the minimum closest value to the supported CLs from `stats[\"max_plan_length\"]`\n",
    "# if max_plan_length is 100, the closest supported CL is 90.\n",
    "# if max_plan_length is 87, the closest supported CL is 52, not 90.\n",
    "supported_cls = [52, 90, 180, 360, 520, 1024, 1536]\n",
    "\n",
    "# Find the largest supported CL that is <= max_plan_length\n",
    "# If all supported CLs are > max_plan_length, choose the smallest supported CL\n",
    "valid_cls = [cl for cl in supported_cls if cl <= stats[\"max_plan_length\"]]\n",
    "if valid_cls:\n",
    "    closest_cl = max(valid_cls)\n",
    "else:\n",
    "    closest_cl = min(supported_cls)\n",
    "\n",
    "print(f\"Max plan length: {stats['max_plan_length']} | Selected context length: {closest_cl}\")\n",
    "\n",
    "# Similarly, for the forecast lengths, we need to get the minimum closest value to the supported FLs from `stats[\"recommended_prediction_length\"]`\n",
    "# if recommended_prediction_length is 100, the closest supported FL is 96.\n",
    "# if recommended_prediction_length is 87, the closest supported FL is 60, not 96.\n",
    "supported_fls = [16, 30, 48, 60, 96, 192, 336, 720]\n",
    "\n",
    "# Find the largest supported FL that is <= recommended_prediction_length\n",
    "# If all supported FLs are > recommended_prediction_length, choose the smallest supported FL\n",
    "valid_fls = [fl for fl in supported_fls if fl <= stats[\"recommended_prediction_length\"]]\n",
    "if valid_fls:\n",
    "    closest_fl = max(valid_fls)\n",
    "else:\n",
    "    closest_fl = min(supported_fls)\n",
    "\n",
    "print(\n",
    "    f\"Recommended prediction length: {stats['recommended_prediction_length']} | Selected forecast length: {closest_fl}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 30922, Val size: 6626, Test size: 6627\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "    dataset_file,\n",
    "    context_length=closest_cl,\n",
    "    prediction_length=closest_fl,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-45880:t-8430149376:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-45880:t-8430149376:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = 52-16-ft-r2.1.\n",
      "INFO:p-45880:t-8430149376:get_model.py:get_model:[TTM] context_length = 52, prediction_length = 16\n",
      "INFO:p-45880:t-8430149376:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received model name: 52-16-ft-r2.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='38660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   47/38660 00:10 < 2:26:32, 4.39 it/s, Epoch 0.02/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m ttm = BlocksWorldTTM(\n\u001b[32m      3\u001b[39m     context_length=closest_cl,\n\u001b[32m      4\u001b[39m     prediction_length=closest_fl,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     num_epochs=\u001b[32m20\u001b[39m,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mttm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mBlocksWorldTTM.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = Trainer(\n\u001b[32m     82\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m     83\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     optimizers=(optimizer, scheduler),\n\u001b[32m     88\u001b[39m )\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:1874\u001b[39m, in \u001b[36mTinyTimeMixerForPrediction.forward\u001b[39m\u001b[34m(self, past_values, future_values, past_observed_mask, future_observed_mask, output_hidden_states, return_loss, return_dict, freq_token, static_categorical_values)\u001b[39m\n\u001b[32m   1871\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_return_dict\n\u001b[32m   1873\u001b[39m \u001b[38;5;66;03m# past_values: tensor [batch_size x context_length x num_input_channels]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1874\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1877\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# model_output: [batch_size x nvars x num_patch x d_model]\u001b[39;00m\n\u001b[32m   1882\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1883\u001b[39m     model_output = TinyTimeMixerModelOutput(*model_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:1621\u001b[39m, in \u001b[36mTinyTimeMixerModel.forward\u001b[39m\u001b[34m(self, past_values, past_observed_mask, output_hidden_states, return_dict, freq_token)\u001b[39m\n\u001b[32m   1617\u001b[39m patched_x = \u001b[38;5;28mself\u001b[39m.patching(scaled_past_values)  \u001b[38;5;66;03m# [batch_size x num_input_channels x num_patch x patch_length\u001b[39;00m\n\u001b[32m   1619\u001b[39m enc_input = patched_x\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m encoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1629\u001b[39m     encoder_output = TinyTimeMixerEncoderOutput(*encoder_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:1522\u001b[39m, in \u001b[36mTinyTimeMixerEncoder.forward\u001b[39m\u001b[34m(self, past_values, output_hidden_states, return_dict, freq_token)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.positional_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1520\u001b[39m     patches = \u001b[38;5;28mself\u001b[39m.positional_encoder(patches)\n\u001b[32m-> \u001b[39m\u001b[32m1522\u001b[39m last_hidden_state, hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp_mixer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m   1526\u001b[39m         v\n\u001b[32m   1527\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m         ]\n\u001b[32m   1531\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:965\u001b[39m, in \u001b[36mTinyTimeMixerBlock.forward\u001b[39m\u001b[34m(self, hidden_state, output_hidden_states)\u001b[39m\n\u001b[32m    963\u001b[39m     all_hidden_states.extend(hidden_states)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     embedding = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    967\u001b[39m         all_hidden_states.append(embedding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:842\u001b[39m, in \u001b[36mTinyTimeMixerLayer.forward\u001b[39m\u001b[34m(self, hidden)\u001b[39m\n\u001b[32m    839\u001b[39m     hidden = \u001b[38;5;28mself\u001b[39m.channel_feature_mixer(hidden)\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_patches > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m     hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_mixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m hidden = \u001b[38;5;28mself\u001b[39m.feature_mixer(hidden)  \u001b[38;5;66;03m# hidden: (batch_size x num_patches x d_model)\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:539\u001b[39m, in \u001b[36mPatchMixerBlock.forward\u001b[39m\u001b[34m(self, hidden_state)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[33;03m    hidden_state (`torch.Tensor`): Input tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    535\u001b[39m \u001b[33;03m    `torch.Tensor`: Transformed tensor.\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    537\u001b[39m residual = hidden_state\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.self_attn:\n\u001b[32m    542\u001b[39m     batch_size, n_vars, num_patches, d_model = hidden_state.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:262\u001b[39m, in \u001b[36mTinyTimeMixerNormLayer.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    259\u001b[39m     inputs = torch.reshape(inputs_reshaped, inputs.shape)\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/usc/aiisc/projects/PaTS/.venv/lib/python3.13/site-packages/torch/nn/modules/normalization.py:216\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    214\u001b[39m             init.zeros_(\u001b[38;5;28mself\u001b[39m.bias)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.layer_norm(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.normalized_shape, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias, \u001b[38;5;28mself\u001b[39m.eps\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextra_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "ttm = BlocksWorldTTM(\n",
    "    context_length=closest_cl,\n",
    "    prediction_length=closest_fl,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=16,\n",
    "    num_epochs=20,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "ttm.train(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & Load Model\n",
    "save_path = f\"../models/blocksworld-{num_blocks}_ttm-{ttm.model_name}\"\n",
    "\n",
    "# Get input from user on the save path if it already exists\n",
    "if os.path.exists(save_path):\n",
    "    user_input = input(f\"Path {save_path} already exists. Overwrite? (y/n): \")\n",
    "    if user_input.lower() != \"y\":\n",
    "        new_path = input(\"Enter a new path: \")\n",
    "        save_path = new_path\n",
    "    else:\n",
    "        print(f\"Overwriting existing path: {save_path}\")\n",
    "else:\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "ttm.save(save_path)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm = BlocksWorldTTM.load(save_path)\n",
    "print(f\"Loaded from {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "metrics = evaluate_model(ttm, test_dataset)\n",
    "\n",
    "# Analyze error patterns\n",
    "print(\"\\nAnalyzing error patterns...\")\n",
    "analysis = analyze_error_patterns(ttm, test_dataset)\n",
    "successes, failures = analysis[\"successes\"], analysis[\"failures\"]\n",
    "\n",
    "# Examine the inital states, goal states, and actions\n",
    "gen = BlocksWorldGenerator(num_blocks=num_blocks)\n",
    "\n",
    "print(\"\\n--------------------------------------------------\")\n",
    "print(f\"Length of the test dataset: {len(test_dataset)}\")\n",
    "print(\"\\nExample Successes:\")\n",
    "for i, case in enumerate(successes[:3]):\n",
    "    print(f\"\\nCase {i + 1}:\")\n",
    "    print(f\"Initial State: {gen.decode_vector_to_blocks(case['initial_state'])}\")\n",
    "    print(f\"Goal State: {gen.decode_vector_to_blocks(case['goal_state'])}\")\n",
    "    print(f\"Predicted Goal: {gen.decode_vector_to_blocks(case['predicted_goal'])}\")\n",
    "\n",
    "print(\"\\nExample Failures:\")\n",
    "for i, case in enumerate(failures[:3]):\n",
    "    print(f\"\\nCase {i + 1}:\")\n",
    "    print(f\"Initial State: {gen.decode_vector_to_blocks(case['initial_state'])}\")\n",
    "    print(f\"Goal State: {gen.decode_vector_to_blocks(case['goal_state'])}\")\n",
    "    print(f\"Predicted Goal: {gen.decode_vector_to_blocks(case['predicted_goal'])}\")\n",
    "    print(f\"Target Goal: {gen.decode_vector_to_blocks(case['target_goal'])}\")\n",
    "    print(f\"Number of Errors: {case['num_errors']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
