{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlocksWorld TTM\n",
    "\n",
    "Use the **2nd Encoding** format to use the TTM Granite model on the BlocksWorld domain.\n",
    "\n",
    "Key modifications from standard TTM:\n",
    "\n",
    "- Input format includes goal state concatenated with current state\n",
    "- Binary state prediction instead of continuous values\n",
    "- Custom metrics for planning success\n",
    "- Sequence padding to handle variable-length plans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import TrackingCallback\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "\n",
    "from BlocksWorld import BlocksWorldGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 13\n",
    "set_seed(SEED)\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Determine device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan Dataclass\n",
    "\n",
    "For storing individual planning examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    context_length: int = 512\n",
    "    prediction_length: int = 96\n",
    "    learning_rate: float = 1e-4\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 50\n",
    "    state_dim: Optional[int] = None  # Will be set during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BlocksWorldSample:\n",
    "    initial_state: List[int]\n",
    "    goal_state: List[int]\n",
    "    plan: List[List[int]]\n",
    "    actions: List[List[str]]\n",
    "    feature_names: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom BlocksWorld Dataset Class\n",
    "\n",
    "The class handles:\n",
    "\n",
    "- Loading JSON plan data\n",
    "- Padding sequences to match context length\n",
    "- Combining state and goal information\n",
    "- Converting to appropriate tensor format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlocksWorldDataset(Dataset):\n",
    "    def __init__(self, data_path: str, context_length: int, prediction_length: int):\n",
    "        self.context_length: int = context_length\n",
    "        self.prediction_length: int = prediction_length\n",
    "        self.device = DEVICE\n",
    "\n",
    "        with open(data_path, \"r\") as f:\n",
    "            raw_data = json.load(f)[\"plans\"]\n",
    "\n",
    "        self.samples: List[BlocksWorldSample] = []\n",
    "        for item in raw_data:\n",
    "            sample = BlocksWorldSample(\n",
    "                initial_state=item[\"initial_state\"],\n",
    "                goal_state=item[\"goal_state\"],\n",
    "                plan=item[\"plan\"],\n",
    "                actions=item[\"actions\"],\n",
    "                feature_names=item[\"feature_names\"],\n",
    "            )\n",
    "            self.samples.append(sample)\n",
    "\n",
    "        # Get dimensionality from first sample\n",
    "        self.state_dim: int = len(self.samples[0].initial_state)\n",
    "\n",
    "    def __len__(self):  # Length of the Dataset\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Pad plan sequence to match context_length + prediction_length\n",
    "        plan_len = len(sample.plan)\n",
    "        full_seq = sample.plan + [sample.goal_state] * (\n",
    "            self.context_length + self.prediction_length - plan_len\n",
    "        )\n",
    "\n",
    "        # Split into past and future\n",
    "        past_seq = full_seq[: self.context_length]\n",
    "        future_seq = full_seq[\n",
    "            self.context_length : self.context_length + self.prediction_length\n",
    "        ]\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        past_values = np.array(past_seq, dtype=np.float32)\n",
    "        future_values = np.array(future_seq, dtype=np.float32)\n",
    "\n",
    "        # Create masks (1 indicates valid values)\n",
    "        past_observed_mask = np.ones(\n",
    "            (self.context_length, self.state_dim), dtype=np.float32\n",
    "        )\n",
    "        future_observed_mask = np.ones(\n",
    "            (self.prediction_length, self.state_dim), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Include goal state as static categorical feature\n",
    "        static_categorical_values = np.array(sample.goal_state, dtype=np.float32)\n",
    "\n",
    "        return {\n",
    "            \"past_values\": torch.tensor(past_values, dtype=torch.float32).to(\n",
    "                self.device\n",
    "            ),\n",
    "            \"future_values\": torch.tensor(future_values, dtype=torch.float32).to(\n",
    "                self.device\n",
    "            ),\n",
    "            \"past_observed_mask\": torch.tensor(\n",
    "                past_observed_mask, dtype=torch.float32\n",
    "            ).to(self.device),\n",
    "            \"future_observed_mask\": torch.tensor(\n",
    "                future_observed_mask, dtype=torch.float32\n",
    "            ).to(self.device),\n",
    "            \"static_categorical_values\": torch.tensor(\n",
    "                static_categorical_values, dtype=torch.float32\n",
    "            ).to(self.device),\n",
    "            \"freq_token\": torch.zeros(1, dtype=torch.long).to(\n",
    "                self.device\n",
    "            ),  # Placeholder for TTM\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why we need padding?**\n",
    "The model expects every input sequence to be exactly `context_length` timesteps long, but our planning sequences can vary in length (some plans take 3 steps, others might take 10). So, the padding strategy is, (1) when plan is too short, pad with the goal state or (2) when plan is too long, truncate to context_length.\n",
    "\n",
    "For example, if we have:\n",
    "\n",
    "```python\n",
    "context_length = 5\n",
    "plan = [[1,0,0], [1,1,0], [0,1,1]]  # 3 steps\n",
    "goal_state = [0,1,1]\n",
    "```\n",
    "\n",
    "After padding:\n",
    "\n",
    "```python\n",
    "padded_plan = [\n",
    "    [1,0,0],    # Original step 1\n",
    "    [1,1,0],    # Original step 2\n",
    "    [0,1,1],    # Original step 3\n",
    "    [0,1,1],    # Padded with goal\n",
    "    [0,1,1]     # Padded with goal\n",
    "]\n",
    "```\n",
    "\n",
    "Why pad with goal state instead of zeros?:\n",
    "\n",
    "1. **Semantic Meaning**: Using the goal state maintains the logical meaning - \"after reaching the goal, we stay in the goal state\"\n",
    "2. **Learning Signal**: It helps the model understand that reaching the goal is a stable state\n",
    "3. **Consistency**: Ensures all states in the sequence are valid block configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlocksWorld-Based TTM Class\n",
    "\n",
    "To handle training and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlocksWorldTTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_length: int = 512,\n",
    "        prediction_length: int = 96,\n",
    "        learning_rate: float = 1e-4,\n",
    "        batch_size: int = 32,\n",
    "        num_epochs: int = 50,\n",
    "    ):\n",
    "        self.config = ModelConfig(\n",
    "            context_length=context_length,\n",
    "            prediction_length=prediction_length,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "        )\n",
    "        self.device = DEVICE\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def train(self, train_dataset: Dataset, val_dataset: Optional[Dataset] = None):\n",
    "        \"\"\"Train the model on given datasets\"\"\"\n",
    "        # Store state dimension from training data\n",
    "        self.config.state_dim = (\n",
    "            train_dataset.dataset.state_dim\n",
    "            if hasattr(train_dataset, \"dataset\")\n",
    "            else train_dataset.state_dim\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=self.config.context_length,\n",
    "            prediction_length=self.config.prediction_length,\n",
    "            head_dropout=0.1,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"blocks_world_ttm\",\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            evaluation_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            TrackingCallback(),\n",
    "            EarlyStoppingCallback(early_stopping_patience=5),\n",
    "        ]\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.config.learning_rate,\n",
    "            epochs=self.config.num_epochs,\n",
    "            steps_per_epoch=math.ceil(len(train_dataset) / self.config.batch_size),\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict(\n",
    "        self, initial_states: torch.Tensor, goal_states: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Generate action sequences to reach goals from given states\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model needs to be trained or loaded before prediction\")\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_size = initial_states.shape[0]\n",
    "\n",
    "            # Create context sequence by repeating initial states\n",
    "            context_sequence = initial_states.unsqueeze(1).repeat(\n",
    "                1, self.config.context_length, 1\n",
    "            )\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(self.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(self.device),\n",
    "                \"static_categorical_values\": goal_states.to(self.device),\n",
    "                \"freq_token\": torch.zeros(batch_size, dtype=torch.long).to(self.device),\n",
    "            }\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.sigmoid(outputs[0])\n",
    "            predictions = torch.round(predictions)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model weights and configuration\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"No model to save. Train or load a model first.\")\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Save model state\n",
    "        model_path = os.path.join(path, \"model.pt\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        # Save configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(asdict(self.config), f)\n",
    "\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"BlocksWorldTTM\":\n",
    "        \"\"\"Load model weights and configuration\"\"\"\n",
    "        # Load configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_dict = json.load(f)\n",
    "\n",
    "        # Create instance with loaded config\n",
    "        instance = cls(\n",
    "            context_length=config_dict[\"context_length\"],\n",
    "            prediction_length=config_dict[\"prediction_length\"],\n",
    "            learning_rate=config_dict[\"learning_rate\"],\n",
    "            batch_size=config_dict[\"batch_size\"],\n",
    "            num_epochs=config_dict[\"num_epochs\"],\n",
    "        )\n",
    "        instance.config.state_dim = config_dict[\"state_dim\"]\n",
    "\n",
    "        # Initialize and load model\n",
    "        instance.model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=instance.config.context_length,\n",
    "            prediction_length=instance.config.prediction_length,\n",
    "            head_dropout=0.1,\n",
    "        ).to(instance.device)\n",
    "\n",
    "        model_path = os.path.join(path, \"model.pt\")\n",
    "        instance.model.load_state_dict(\n",
    "            torch.load(model_path, map_location=instance.device)\n",
    "        )\n",
    "        instance.model.eval()\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model receives these key components for each sample during **training**:\n",
    "\n",
    "1. Past Values (past_values):\n",
    "\n",
    "   `past_values = torch.tensor(past_seq, dtype=torch.float32)`\n",
    "\n",
    "   - Shape: [batch_size, context_length, state_dim]\n",
    "   - These are sequences of states leading up to the current point\n",
    "   - Each state is a binary vector representing the blocks world predicates\n",
    "   - Length is padded to context_length (512 by default)\n",
    "\n",
    "2. Future Values (future_values):\n",
    "\n",
    "   `future_values = torch.tensor(future_seq, dtype=torch.float32)`\n",
    "\n",
    "   - Shape: [batch_size, prediction_length, state_dim]`\n",
    "   - These are the target sequences of states we want to predict\n",
    "   - Length is padded to prediction_length (96 by default)\n",
    "\n",
    "3. Observation Masks:\n",
    "\n",
    "   `past_observed_mask = torch.ones((context_length, state_dim))`\n",
    "\n",
    "   `future_observed_mask = torch.ones((prediction_length, state_dim))`\n",
    "\n",
    "   - Binary masks indicating which values are valid (1) vs padding (0)\n",
    "   - Helps model ignore padded values during training\n",
    "\n",
    "4. Static Categorical Values:\n",
    "\n",
    "   `static_categorical_values = torch.tensor(sample.goal_state)`\n",
    "\n",
    "   - Shape: [batch_size, state_dim]\n",
    "   - The goal state we want to reach\n",
    "   - Stays constant across the entire sequence\n",
    "   - Helps guide the prediction towards the goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Prediction the model takes:\n",
    "\n",
    "1. Initial States:\n",
    "\n",
    "   `past_values = torch.tensor(initial_states).unsqueeze(1).repeat(1, context_length, 1)`\n",
    "\n",
    "   - The starting state is repeated to fill the context window\n",
    "   - This gives the model the current state as context\n",
    "\n",
    "2. Goal States:\n",
    "\n",
    "   `static_categorical_values = torch.tensor(goal_states)`\n",
    "\n",
    "   - Target goal state as static features\n",
    "   - Guides the generation of the plan\n",
    "\n",
    "The model outputs:\n",
    "\n",
    "```\n",
    "predictions = torch.sigmoid(outputs[0])  # Convert to probabilities\n",
    "predictions = torch.round(predictions)   # Convert to binary states\n",
    "```\n",
    "\n",
    "- Shape: [batch_size, prediction_length, state_dim]\n",
    "- Sequence of predicted states forming a plan\n",
    "- Each state is a binary vector matching the input encoding\n",
    "- The sequence should transition from initial state to goal state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data_path: str, context_length: int, prediction_length: int):\n",
    "    \"\"\"Create train/val/test datasets\"\"\"\n",
    "    full_dataset = BlocksWorldDataset(data_path, context_length, prediction_length)\n",
    "\n",
    "    # Split indices\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.15 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(SEED),\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, targets):\n",
    "    \"\"\"Compute metrics for predicted plans\"\"\"\n",
    "    predictions = predictions.numpy()\n",
    "    targets = targets.numpy()\n",
    "\n",
    "    # State prediction accuracy\n",
    "    state_accuracy = np.mean(predictions == targets)\n",
    "\n",
    "    # Goal achievement rate (exact match of final state)\n",
    "    goal_achieved = np.all(predictions[:, -1] == targets[:, -1], axis=1)\n",
    "    goal_achievement_rate = np.mean(goal_achieved)\n",
    "\n",
    "    # Partial goal achievement (percentage of correct final state bits)\n",
    "    partial_goal = np.mean(predictions[:, -1] == targets[:, -1], axis=1)\n",
    "    avg_partial_goal = np.mean(partial_goal)\n",
    "\n",
    "    return {\n",
    "        \"state_accuracy\": state_accuracy,\n",
    "        \"goal_achievement_rate\": goal_achievement_rate,\n",
    "        \"avg_partial_goal\": avg_partial_goal,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the model with more detailed metrics\n",
    "    \"\"\"\n",
    "    model.model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    goal_state_predictions = []\n",
    "    goal_state_targets = []\n",
    "\n",
    "    num_samples = len(test_dataset)\n",
    "    num_exact_matches = 0\n",
    "    num_partial_matches = 0\n",
    "    total_bits_correct = 0\n",
    "    total_bits = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            sample = test_dataset[i]\n",
    "\n",
    "            # Get initial and goal states\n",
    "            initial_state = sample[\"past_values\"][0]\n",
    "            goal_state = sample[\"static_categorical_values\"]\n",
    "            target = sample[\"future_values\"]\n",
    "\n",
    "            # Create context sequence\n",
    "            context_sequence = initial_state.unsqueeze(0).repeat(\n",
    "                1, model.config.context_length, 1\n",
    "            )\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(model.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(\n",
    "                    model.device\n",
    "                ),\n",
    "                \"static_categorical_values\": goal_state.unsqueeze(0).to(model.device),\n",
    "                \"freq_token\": torch.zeros(1, dtype=torch.long).to(model.device),\n",
    "            }\n",
    "\n",
    "            # Get prediction\n",
    "            outputs = model.model(**inputs)\n",
    "            prediction = torch.sigmoid(outputs[0])\n",
    "            prediction = torch.round(prediction)\n",
    "\n",
    "            # Store predictions and targets\n",
    "            all_predictions.append(prediction)\n",
    "            all_targets.append(target)\n",
    "\n",
    "            # Focus on goal states (final states)\n",
    "            pred_goal = prediction[0, -1]\n",
    "            true_goal = target[-1]\n",
    "\n",
    "            goal_state_predictions.append(pred_goal)\n",
    "            goal_state_targets.append(true_goal)\n",
    "\n",
    "            # Calculate exact matches\n",
    "            if torch.all(pred_goal == true_goal):\n",
    "                num_exact_matches += 1\n",
    "\n",
    "            # Calculate partial matches (more than 50% bits correct)\n",
    "            num_correct_bits = torch.sum(pred_goal == true_goal).item()\n",
    "            total_bits_correct += num_correct_bits\n",
    "            total_bits += len(pred_goal)\n",
    "\n",
    "            if num_correct_bits > len(pred_goal) / 2:\n",
    "                num_partial_matches += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"num_samples\": num_samples,\n",
    "        \"num_exact_matches\": num_exact_matches,\n",
    "        \"exact_match_rate\": num_exact_matches / num_samples,\n",
    "        \"num_partial_matches\": num_partial_matches,\n",
    "        \"partial_match_rate\": num_partial_matches / num_samples,\n",
    "        \"bit_accuracy\": total_bits_correct / total_bits,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nDetailed Model Evaluation Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total number of test samples: {metrics['num_samples']}\")\n",
    "        print(f\"Number of exact goal state matches: {metrics['num_exact_matches']}\")\n",
    "        print(f\"Exact match rate: {metrics['exact_match_rate']:.4f}\")\n",
    "        print(\n",
    "            f\"Number of partial matches (>50% correct): {metrics['num_partial_matches']}\"\n",
    "        )\n",
    "        print(f\"Partial match rate: {metrics['partial_match_rate']:.4f}\")\n",
    "        print(f\"Bit-level accuracy: {metrics['bit_accuracy']:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def analyze_error_patterns(model, test_dataset, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced error pattern analysis with more detailed statistics\n",
    "    \"\"\"\n",
    "    model.model.eval()\n",
    "    successes = []\n",
    "    failures = []\n",
    "\n",
    "    bit_error_counts = {}  # Track which bits are most commonly wrong\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "\n",
    "            # Get initial and goal states\n",
    "            initial_state = sample[\"past_values\"][0]\n",
    "            goal_state = sample[\"static_categorical_values\"]\n",
    "            target = sample[\"future_values\"][-1]\n",
    "\n",
    "            # Create context sequence\n",
    "            context_sequence = initial_state.unsqueeze(0).repeat(\n",
    "                1, model.config.context_length, 1\n",
    "            )\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"past_values\": context_sequence.to(model.device),\n",
    "                \"past_observed_mask\": torch.ones_like(context_sequence).to(\n",
    "                    model.device\n",
    "                ),\n",
    "                \"static_categorical_values\": goal_state.unsqueeze(0).to(model.device),\n",
    "                \"freq_token\": torch.zeros(1, dtype=torch.long).to(model.device),\n",
    "            }\n",
    "\n",
    "            # Get prediction\n",
    "            outputs = model.model(**inputs)\n",
    "            prediction = torch.sigmoid(outputs[0])\n",
    "            prediction = torch.round(prediction)\n",
    "            predicted_goal = prediction[0, -1]\n",
    "\n",
    "            # Calculate error statistics\n",
    "            errors = (predicted_goal != target).nonzero().squeeze(1)\n",
    "            num_errors = len(errors)\n",
    "\n",
    "            # Track which bits had errors\n",
    "            for error_idx in errors:\n",
    "                if error_idx.item() not in bit_error_counts:\n",
    "                    bit_error_counts[error_idx.item()] = 0\n",
    "                bit_error_counts[error_idx.item()] += 1\n",
    "\n",
    "            case = {\n",
    "                \"initial_state\": initial_state.cpu().numpy(),\n",
    "                \"goal_state\": goal_state.cpu().numpy(),\n",
    "                \"predicted_goal\": predicted_goal.cpu().numpy(),\n",
    "                \"target_goal\": target.cpu().numpy(),\n",
    "                \"num_errors\": num_errors,\n",
    "                \"error_positions\": errors.cpu().numpy(),\n",
    "            }\n",
    "\n",
    "            if num_errors == 0:\n",
    "                successes.append(case)\n",
    "            else:\n",
    "                failures.append(case)\n",
    "\n",
    "    analysis = {\n",
    "        \"num_successes\": len(successes),\n",
    "        \"num_failures\": len(failures),\n",
    "        \"success_rate\": len(successes) / (len(successes) + len(failures)),\n",
    "        \"bit_error_counts\": bit_error_counts,\n",
    "        \"successes\": successes,\n",
    "        \"failures\": failures,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nError Pattern Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of successful predictions: {analysis['num_successes']}\")\n",
    "        print(f\"Number of failed predictions: {analysis['num_failures']}\")\n",
    "        print(f\"Success rate: {analysis['success_rate']:.4f}\")\n",
    "        print(\"\\nMost common error positions:\")\n",
    "        sorted_errors = sorted(\n",
    "            bit_error_counts.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        for bit, count in sorted_errors[:5]:\n",
    "            print(f\"Bit {bit}: {count} errors\")\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks in the dataset: 4\n",
      "Train size: 819, Val size: 175, Test size: 176\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "dataset_file = \"../data/dataset_4.json\"\n",
    "print(\n",
    "    f\"Number of blocks in the dataset: {(num_blocks := int(dataset_file.split('_')[-1][0]))}\"\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "    dataset_file, context_length=512, prediction_length=96\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(data_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze the dataset to determine appropriate parameters\"\"\"\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)[\"plans\"]\n",
    "\n",
    "    # Get key statistics\n",
    "    max_plan_length = max(len(item[\"plan\"]) for item in data)\n",
    "    avg_plan_length = sum(len(item[\"plan\"]) for item in data) / len(data)\n",
    "    state_dim = len(data[0][\"initial_state\"])\n",
    "    num_samples = len(data)\n",
    "\n",
    "    stats = {\n",
    "        \"max_plan_length\": max_plan_length,\n",
    "        \"avg_plan_length\": avg_plan_length,\n",
    "        \"state_dim\": state_dim,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"recommended_prediction_length\": max_plan_length + 2,  # Add small buffer\n",
    "    }\n",
    "\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Maximum plan length: {max_plan_length}\")\n",
    "    print(f\"Average plan length: {avg_plan_length:.2f}\")\n",
    "    print(f\"Recommended prediction length: {stats['recommended_prediction_length']}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Number of samples: 1170\n",
      "State dimension: 24\n",
      "Maximum plan length: 13\n",
      "Average plan length: 6.74\n",
      "Recommended prediction length: 15\n"
     ]
    }
   ],
   "source": [
    "# Analyze dataset\n",
    "stats = analyze_dataset(dataset_file)\n",
    "\n",
    "# Use recommended prediction length\n",
    "prediction_length = stats[\"recommended_prediction_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-60977:t-8250786368:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "INFO:p-60977:t-8250786368:get_model.py:get_model:Selected prediction_length = 96\n",
      "WARNING:p-60977:t-8250786368:get_model.py:get_model:Requested `prediction_length` (15) is not exactly equal to any of the available TTM prediction lengths.\n",
      "                    Hence, TTM will forecast using the `prediction_filter_length` argument to provide the requested prediction length.\n",
      "                    Supported context lengths (CL) and forecast/prediction lengths (FL) for Model Card: ibm-granite/granite-timeseries-ttm-r2 are\n",
      "                    {'CL': [512, 1024, 1536], 'FL': [96, 192, 336, 720]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-60977:t-8250786368:get_model.py:get_model:Model loaded successfully!\n",
      "INFO:p-60977:t-8250786368:get_model.py:get_model:[TTM] context_len = 512, forecast_len = 96\n",
      "/Users/nitingupta/miniconda3/envs/tsplans/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d1951cddad4aec8dec83d8742c0f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1797c4803f05417c8e96ee5aa6103466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6901898334253929e-06, 'eval_runtime': 2.7129, 'eval_samples_per_second': 64.506, 'eval_steps_per_second': 2.212, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ab2f197713421085deb7761f0507b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.151666992882383e-06, 'eval_runtime': 2.5197, 'eval_samples_per_second': 69.453, 'eval_steps_per_second': 2.381, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657d969788fc463a87a14081d7b08a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.53977906495129e-07, 'eval_runtime': 1.5227, 'eval_samples_per_second': 114.929, 'eval_steps_per_second': 3.94, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf7edbfc9394b0d9e5cad4f6e82b50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.498484997839114e-07, 'eval_runtime': 1.5651, 'eval_samples_per_second': 111.816, 'eval_steps_per_second': 3.834, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a07c6d87e3b4f6cb1c7a2df7ce78864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5222836913817446e-07, 'eval_runtime': 1.5548, 'eval_samples_per_second': 112.554, 'eval_steps_per_second': 3.859, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aaba2036ca447e9cc1f445e57c3f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9028532821939734e-07, 'eval_runtime': 3.9475, 'eval_samples_per_second': 44.331, 'eval_steps_per_second': 1.52, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0814c03758b43c6b67ea02aae6b9d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.161716139426062e-07, 'eval_runtime': 1.7425, 'eval_samples_per_second': 100.433, 'eval_steps_per_second': 3.443, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8b6a7f8f964729ad699e1ceebe1f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7029989862749062e-07, 'eval_runtime': 2.3768, 'eval_samples_per_second': 73.628, 'eval_steps_per_second': 2.524, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3704a6f340e9489eb1e9886434dbd275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4816184545907163e-07, 'eval_runtime': 2.8574, 'eval_samples_per_second': 61.245, 'eval_steps_per_second': 2.1, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b58d2e42834c24824c52a0d5e5b266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4930454028672102e-07, 'eval_runtime': 1.629, 'eval_samples_per_second': 107.425, 'eval_steps_per_second': 3.683, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494f9061f7ec4bdab4e23eb32544b962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3119922925852734e-07, 'eval_runtime': 1.6125, 'eval_samples_per_second': 108.527, 'eval_steps_per_second': 3.721, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f8a395874842c1a87c9398a54efa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3062449966128042e-07, 'eval_runtime': 3.2311, 'eval_samples_per_second': 54.16, 'eval_steps_per_second': 1.857, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575d60dabc644a0c86170b23abefbad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.253309229961815e-08, 'eval_runtime': 3.2578, 'eval_samples_per_second': 53.718, 'eval_steps_per_second': 1.842, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d6366b991d48b09ff4a7620880a966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.506955223059776e-08, 'eval_runtime': 1.64, 'eval_samples_per_second': 106.705, 'eval_steps_per_second': 3.658, 'epoch': 14.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6521787df4ca4df5aea7b7a806f6fb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.165039678762696e-08, 'eval_runtime': 1.5129, 'eval_samples_per_second': 115.673, 'eval_steps_per_second': 3.966, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a43afe0ecd34af9b7fea90dbafc054b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.827023461639328e-08, 'eval_runtime': 1.5147, 'eval_samples_per_second': 115.535, 'eval_steps_per_second': 3.961, 'epoch': 16.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652d9d7433064d1e9e7e6370721cdb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.094189165471107e-08, 'eval_runtime': 1.5094, 'eval_samples_per_second': 115.94, 'eval_steps_per_second': 3.975, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e66116df3541b28a4e6e936ba65363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.267307929239905e-08, 'eval_runtime': 1.511, 'eval_samples_per_second': 115.82, 'eval_steps_per_second': 3.971, 'epoch': 18.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf0e31680b7466abc819f54e5000986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0307333531709446e-08, 'eval_runtime': 1.5708, 'eval_samples_per_second': 111.408, 'eval_steps_per_second': 3.82, 'epoch': 19.0}\n",
      "{'loss': 0.0, 'grad_norm': 2.596944432298187e-05, 'learning_rate': 9.637356308122499e-05, 'epoch': 19.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3828a2330a477cb2003e6f3685df37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.481585153508604e-08, 'eval_runtime': 1.5744, 'eval_samples_per_second': 111.154, 'eval_steps_per_second': 3.811, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee156725f974444b2bd53fbab27edad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.320680891898519e-08, 'eval_runtime': 1.5619, 'eval_samples_per_second': 112.042, 'eval_steps_per_second': 3.841, 'epoch': 21.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc56fdc3744543b5b233eaa18204f1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.110114832338695e-08, 'eval_runtime': 1.6315, 'eval_samples_per_second': 107.265, 'eval_steps_per_second': 3.678, 'epoch': 22.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b7ff8ac00b40f18841524c0ab17b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5150677984697722e-08, 'eval_runtime': 1.6793, 'eval_samples_per_second': 104.21, 'eval_steps_per_second': 3.573, 'epoch': 23.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e437d8f19fb44f3caadb73f93959f109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4613598270661896e-08, 'eval_runtime': 1.4958, 'eval_samples_per_second': 116.996, 'eval_steps_per_second': 4.011, 'epoch': 24.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d776213e9de74f9695eced75a583cf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6928352053469098e-08, 'eval_runtime': 1.7241, 'eval_samples_per_second': 101.504, 'eval_steps_per_second': 3.48, 'epoch': 25.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c087aebcb40f41f4b91a17a5add9a366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6062177365938624e-08, 'eval_runtime': 1.592, 'eval_samples_per_second': 109.924, 'eval_steps_per_second': 3.769, 'epoch': 26.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3274fccf4144a9bfc01e10b5f52a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6902239486380495e-08, 'eval_runtime': 1.6753, 'eval_samples_per_second': 104.46, 'eval_steps_per_second': 3.581, 'epoch': 27.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d1145995db473d9a10061f59f0c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7278143360499598e-08, 'eval_runtime': 1.544, 'eval_samples_per_second': 113.345, 'eval_steps_per_second': 3.886, 'epoch': 28.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c1fc50dbea43d39e948b207b8476dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6714926331928837e-08, 'eval_runtime': 1.8055, 'eval_samples_per_second': 96.926, 'eval_steps_per_second': 3.323, 'epoch': 29.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1802f90d404d411698b05b0548813fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4325819641669568e-08, 'eval_runtime': 1.4891, 'eval_samples_per_second': 117.523, 'eval_steps_per_second': 4.029, 'epoch': 30.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d22203d3254646b0d0d542cba95cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6082077891610425e-08, 'eval_runtime': 2.3329, 'eval_samples_per_second': 75.014, 'eval_steps_per_second': 2.572, 'epoch': 31.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932b619a699e423faa544c90d105e68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1455193177644105e-08, 'eval_runtime': 30.5739, 'eval_samples_per_second': 5.724, 'eval_steps_per_second': 0.196, 'epoch': 32.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31ec063c345458aa8cf514df44ccece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3942916154974228e-08, 'eval_runtime': 1.851, 'eval_samples_per_second': 94.545, 'eval_steps_per_second': 3.242, 'epoch': 33.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254f9735f238421d9e00d36e8972e539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.547094896636736e-08, 'eval_runtime': 1.5481, 'eval_samples_per_second': 113.045, 'eval_steps_per_second': 3.876, 'epoch': 34.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c6f7f148d0424fbc1470d59b321030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5845284195847853e-08, 'eval_runtime': 16.1854, 'eval_samples_per_second': 10.812, 'eval_steps_per_second': 0.371, 'epoch': 35.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12275e4618c4ea3a4aaca080a0ba0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.054646059372999e-08, 'eval_runtime': 1.4765, 'eval_samples_per_second': 118.525, 'eval_steps_per_second': 4.064, 'epoch': 36.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24478e131a7a42508b05a7e163eabab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2288955630879173e-08, 'eval_runtime': 1.6757, 'eval_samples_per_second': 104.435, 'eval_steps_per_second': 3.581, 'epoch': 37.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f06225f540f495d94d5eeb02d27d160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0663200100680115e-08, 'eval_runtime': 4.2298, 'eval_samples_per_second': 41.373, 'eval_steps_per_second': 1.419, 'epoch': 38.0}\n",
      "{'loss': 0.0, 'grad_norm': 1.084436007658951e-05, 'learning_rate': 2.4355338709560175e-05, 'epoch': 38.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ece05a35bc74c06953550260b3af314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.059168663886112e-08, 'eval_runtime': 2.0414, 'eval_samples_per_second': 85.724, 'eval_steps_per_second': 2.939, 'epoch': 39.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a2407347744a54a8f36bea0ba75653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1189190907145985e-08, 'eval_runtime': 3.3692, 'eval_samples_per_second': 51.941, 'eval_steps_per_second': 1.781, 'epoch': 40.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e6a4429b32481e9cf76b8863984b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.817452628624324e-09, 'eval_runtime': 1.5412, 'eval_samples_per_second': 113.551, 'eval_steps_per_second': 3.893, 'epoch': 41.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c14e7e56b94c968cd1f42a70ce7353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0438124142808647e-08, 'eval_runtime': 1.6927, 'eval_samples_per_second': 103.385, 'eval_steps_per_second': 3.545, 'epoch': 42.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f782a7841b244382bd9e4451796cff51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1224943641252594e-08, 'eval_runtime': 1.6129, 'eval_samples_per_second': 108.502, 'eval_steps_per_second': 3.72, 'epoch': 43.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dbc8f1ce6042e6b123794364437040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0711632469906363e-08, 'eval_runtime': 1.8045, 'eval_samples_per_second': 96.977, 'eval_steps_per_second': 3.325, 'epoch': 44.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79624786c1845298a95249d4a5f5226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.050291764670419e-08, 'eval_runtime': 3.6182, 'eval_samples_per_second': 48.367, 'eval_steps_per_second': 1.658, 'epoch': 45.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66efd9ca3a0948a4bcc46706aed63370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0538242278812504e-08, 'eval_runtime': 1.6088, 'eval_samples_per_second': 108.774, 'eval_steps_per_second': 3.729, 'epoch': 46.0}\n",
      "{'train_runtime': 5887.2624, 'train_samples_per_second': 6.956, 'train_steps_per_second': 0.221, 'train_loss': 1.4204963805472453e-06, 'epoch': 46.0}\n",
      "[TrackingCallback] Mean Epoch Time = 124.8300154209137 seconds, Total Train Time = 5887.263745069504\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "ttm = BlocksWorldTTM(\n",
    "    context_length=512,\n",
    "    prediction_length=prediction_length,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=32,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "ttm.train(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/blocks_world_ttm_4\n",
      "Saved to ../models/blocks_world_ttm_4\n"
     ]
    }
   ],
   "source": [
    "# Save & Load Model\n",
    "save_path = f\"../models/blocks_world_ttm_{num_blocks}\"\n",
    "\n",
    "ttm.save(save_path)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-60977:t-8250786368:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "INFO:p-60977:t-8250786368:get_model.py:get_model:Selected prediction_length = 96\n",
      "WARNING:p-60977:t-8250786368:get_model.py:get_model:Requested `prediction_length` (15) is not exactly equal to any of the available TTM prediction lengths.\n",
      "                    Hence, TTM will forecast using the `prediction_filter_length` argument to provide the requested prediction length.\n",
      "                    Supported context lengths (CL) and forecast/prediction lengths (FL) for Model Card: ibm-granite/granite-timeseries-ttm-r2 are\n",
      "                    {'CL': [512, 1024, 1536], 'FL': [96, 192, 336, 720]}\n",
      "INFO:p-60977:t-8250786368:get_model.py:get_model:Model loaded successfully!\n",
      "INFO:p-60977:t-8250786368:get_model.py:get_model:[TTM] context_len = 512, forecast_len = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/blocks_world_ttm_4\n",
      "Loaded from ../models/blocks_world_ttm_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/q_twsl9n5lg8f5977cn3v4jh0000gn/T/ipykernel_60977/617003521.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  instance.model.load_state_dict(torch.load(model_path, map_location=instance.device))\n"
     ]
    }
   ],
   "source": [
    "ttm = BlocksWorldTTM.load(save_path)\n",
    "print(f\"Loaded from {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Detailed Model Evaluation Metrics:\n",
      "--------------------------------------------------\n",
      "Total number of test samples: 176\n",
      "Number of exact goal state matches: 7\n",
      "Exact match rate: 0.0398\n",
      "Number of partial matches (>50% correct): 176\n",
      "Partial match rate: 1.0000\n",
      "Bit-level accuracy: 0.7434\n",
      "\n",
      "Analyzing error patterns...\n",
      "\n",
      "Error Pattern Analysis:\n",
      "--------------------------------------------------\n",
      "Number of successful predictions: 7\n",
      "Number of failed predictions: 169\n",
      "Success rate: 0.0398\n",
      "\n",
      "Most common error positions:\n",
      "Bit 17: 95 errors\n",
      "Bit 19: 94 errors\n",
      "Bit 3: 85 errors\n",
      "Bit 1: 80 errors\n",
      "Bit 16: 80 errors\n",
      "\n",
      "--------------------------------------------------\n",
      "Length of the test dataset: 176\n",
      "\n",
      "Example Successes:\n",
      "\n",
      "Case 1:\n",
      "Initial State: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'D', 'C': 'A', 'D': 'C'}, holding=None)\n",
      "Goal State: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'D', 'C': 'A', 'D': 'C'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'D', 'C': 'A', 'D': 'C'}, holding=None)\n",
      "\n",
      "Case 2:\n",
      "Initial State: BlockState(clear={'A', 'D'}, on_table={'A', 'B'}, on={'C': 'B', 'D': 'C'}, holding=None)\n",
      "Goal State: BlockState(clear={'A', 'D'}, on_table={'A', 'B'}, on={'C': 'B', 'D': 'C'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'A', 'D'}, on_table={'A', 'B'}, on={'C': 'B', 'D': 'C'}, holding=None)\n",
      "\n",
      "Case 3:\n",
      "Initial State: BlockState(clear={'D'}, on_table={'A'}, on={'B': 'C', 'C': 'B', 'D': 'A'}, holding=None)\n",
      "Goal State: BlockState(clear={'D'}, on_table={'A'}, on={'B': 'C', 'C': 'B', 'D': 'A'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'D'}, on_table={'A'}, on={'B': 'C', 'C': 'B', 'D': 'A'}, holding=None)\n",
      "\n",
      "Example Failures:\n",
      "\n",
      "Case 1:\n",
      "Initial State: BlockState(clear={'C', 'B'}, on_table={'A', 'B'}, on={'C': 'D', 'D': 'A'}, holding=None)\n",
      "Goal State: BlockState(clear={'A', 'B', 'D'}, on_table={'C', 'A', 'D'}, on={'B': 'C'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'C', 'B'}, on_table={'A', 'B'}, on={'C': 'D', 'D': 'A'}, holding=None)\n",
      "Target Goal: BlockState(clear={'A', 'B', 'D'}, on_table={'C', 'A', 'D'}, on={'B': 'C'}, holding=None)\n",
      "Number of Errors: 9\n",
      "\n",
      "Case 2:\n",
      "Initial State: BlockState(clear={'A', 'B'}, on_table={'A', 'D'}, on={'B': 'C', 'C': 'D'}, holding=None)\n",
      "Goal State: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'D', 'C': 'A', 'D': 'C'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'A', 'B'}, on_table={'A', 'D'}, on={'B': 'C', 'C': 'D'}, holding=None)\n",
      "Target Goal: BlockState(clear={'B'}, on_table={'A'}, on={'B': 'D', 'C': 'A', 'D': 'C'}, holding=None)\n",
      "Number of Errors: 7\n",
      "\n",
      "Case 3:\n",
      "Initial State: BlockState(clear={'C', 'A', 'B', 'D'}, on_table={'C', 'A', 'B', 'D'}, on={}, holding=None)\n",
      "Goal State: BlockState(clear={'C', 'B'}, on_table={'A', 'D'}, on={'B': 'A', 'C': 'D'}, holding=None)\n",
      "Predicted Goal: BlockState(clear={'C', 'A', 'B', 'D'}, on_table={'C', 'A', 'B', 'D'}, on={}, holding=None)\n",
      "Target Goal: BlockState(clear={'C', 'B'}, on_table={'A', 'D'}, on={'B': 'A', 'C': 'D'}, holding=None)\n",
      "Number of Errors: 6\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "metrics = evaluate_model(ttm, test_dataset)\n",
    "\n",
    "# Analyze error patterns\n",
    "print(\"\\nAnalyzing error patterns...\")\n",
    "analysis = analyze_error_patterns(ttm, test_dataset)\n",
    "successes, failures = analysis[\"successes\"], analysis[\"failures\"]\n",
    "\n",
    "# Examine the inital states, goal states, and actions\n",
    "gen = BlocksWorldGenerator(num_blocks=num_blocks)\n",
    "\n",
    "print(\"\\n--------------------------------------------------\")\n",
    "print(f\"Length of the test dataset: {len(test_dataset)}\")\n",
    "print(\"\\nExample Successes:\")\n",
    "for i, case in enumerate(successes[:3]):\n",
    "    print(f\"\\nCase {i + 1}:\")\n",
    "    print(f\"Initial State: {gen.decode_vector_to_blocks(case['initial_state'])}\")\n",
    "    print(f\"Goal State: {gen.decode_vector_to_blocks(case['goal_state'])}\")\n",
    "    print(f\"Predicted Goal: {gen.decode_vector_to_blocks(case['predicted_goal'])}\")\n",
    "\n",
    "print(\"\\nExample Failures:\")\n",
    "for i, case in enumerate(failures[:3]):\n",
    "    print(f\"\\nCase {i + 1}:\")\n",
    "    print(f\"Initial State: {gen.decode_vector_to_blocks(case['initial_state'])}\")\n",
    "    print(f\"Goal State: {gen.decode_vector_to_blocks(case['goal_state'])}\")\n",
    "    print(f\"Predicted Goal: {gen.decode_vector_to_blocks(case['predicted_goal'])}\")\n",
    "    print(f\"Target Goal: {gen.decode_vector_to_blocks(case['target_goal'])}\")\n",
    "    print(f\"Number of Errors: {case['num_errors']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsplans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
