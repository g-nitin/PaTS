## Dataset

The dataset for PaTS consists of solved planning problem instances from the Blocksworld domain. Each instance includes the problem definition, the expert plan, and the full state trajectory from the initial state to the goal state.

### Generation Workflow

The dataset generation process is designed for efficiency and scalability. It follows a multi-stage workflow that separates expensive, encoding-agnostic computations from the lightweight, encoding-specific processing. This ensures that PDDL generation, planning, and validation are performed only **once** per problem, even when generating data for multiple state encodings.

The `data/generate_dataset.sh` script automates this entire pipeline:

1.  **Stage 1: Encoding-Agnostic Generation (The Heavy Lifting)**
    *   **Problem Generation**: Creates PDDL problem files (`.pddl`) using a domain-specific generator. Here we call to the [pddl-generators](https://github.com/AI-Planning/pddl-generators/tree/main) repository.
    *   **Plan Generation**: Uses the [Fast Downward planner](https://github.com/aibasel/downward) to find an optimal solution (`.plan`) for each problem. Problems that cannot be solved within the time limit are discarded.
    *   **Plan Validation**: Uses [VAL](https://github.com/KCL-Planning/VAL) to validate the generated plan and produce a verbose log of all state changes (`.val.log`). Invalid plans result in the problem being discarded.

2.  **Stage 2: State Trajectory Extraction**
    *   The `data/parse_and_encode.py` script is called for the first time to process the VAL log and reconstruct the full sequence of states in a human-readable text format (`.traj.txt`). This text-based trajectory becomes the single source of truth for a given problem.

3.  **Stage 3: Uniqueness Filtering**
    *   To prevent data leakage between training and test sets, a unique hash is generated for each (initial state, goal state) pair. If a newly generated problem is a duplicate of one already processed, it is discarded.

4.  **Stage 4: Multi-Encoding Processing**
    *   The script then **loops through all desired encoding types** (e.g., `bin`, `sas`).
    *   For each encoding, it re-invokes `data/parse_and_encode.py`. The script reads the intermediate text trajectory and converts it into the specified numerical vector format, saving the results as NumPy arrays (`.npy`). This step is very fast as it involves no planning or simulation.

5.  **Stage 5: Dataset Splitting**
    *   Finally, `data/analyze_dataset_splits.py` analyzes the distribution of plan lengths across all successfully generated unique problems. It then creates stratified `train_files.txt`, `val_files.txt`, and `test_files.txt`, ensuring that all instances of a unique (initial, goal) pair are kept within the same split.

### Data Structure and Format

The dataset is organized into two distinct top-level directories: `raw_problems` for encoding-agnostic source files and `processed_trajectories` for the final numerical data.

#### `data/raw_problems/<domain_name>/N<num_blocks>/`

This directory contains all the raw, **encoding-agnostic** data. It serves as the foundation for all subsequent encoding steps.

-   `pddl/`: PDDL problem definition files (`.pddl`).
-   `plans/`: Plan files generated by Fast Downward (`.plan`).
-   `val_out/`: Verbose output from VAL (`.val.log`).
-   `trajectories_text/`: Human-readable text representation of state trajectories (`.traj.txt`). This is the intermediate representation used for all encodings.
-   `splits/`: Contains the data splits and analysis artifacts.
    -   `train_files.txt`, `val_files.txt`, `test_files.txt`: Lists of problem basenames for each data split.
    -   `plan_length_distribution_*.png`: Plots visualizing the plan length distributions.

#### `data/processed_trajectories/<domain_name>/N<num_blocks>/<encoding_type>/`

This directory contains the final, **encoding-specific** numerical representations of state trajectories and their associated metadata. Each encoding type (`bin`, `sas`, etc.) gets its own subdirectory.

-   `encoding_info.json`: **Crucial file** describing the encoding used (type, feature dimension, block order, etc.). This file makes each processed dataset self-describing.
-   `predicate_manifest.txt`: For `bin` encoding only, this lists all predicates in order, defining the feature map.
-   `blocks_<N>_problem_<M>.traj.<encoding>.npy`: NumPy array of shape `(L, F)` containing the encoded state trajectory, where `L` is the plan length + 1 and `F` is the feature dimension.
-   `blocks_<N>_problem_<M>.goal.<encoding>.npy`: NumPy array of shape `(F,)` representing the encoded goal state.

### State Encoding

PaTS supports multiple state encoding schemes, controlled by the `--encoding_type` flag in `parse_and_encode.py`.

#### Binary Predicate Encoding (`bin`)

-   **Representation**: A sparse binary vector where each element corresponds to a ground predicate (e.g., `(on-table b1)`). A value of `1` means the predicate is true, and `0` means it is false.
-   **Size**: Scales quadratically with the number of blocks, O(nÂ²).
-   **Configuration**: The structure is defined by `predicate_manifest.txt` located within the `.../bin/` directory. The corresponding `encoding_info.json` file specifies the type as `bin` and points to this manifest.

#### SAS+-like Position Vector Encoding (`sas`)

-   **Representation**: A dense integer vector where the _index_ represents a block and the _value_ represents its position. This is a multi-valued representation inspired by SAS+ formalisms.
    -   `vector[i]` corresponds to block `b(i+1)`.
    -   Value `0`: The block is on the table.
    -   Value `j > 0`: The block is on top of block `bj`.
    -   Value `-1`: The block is being held by the arm.
-   **Example (4 blocks)**: For the state "A on B, B on table, C on D, D on table" (with A=b1, B=b2, C=b3, D=b4), the encoding is `[2, 0, 4, 0]`.
-   **Size**: Scales linearly with the number of blocks, O(n).
-   **Configuration**: The `encoding_info.json` file, located in the `.../sas/` directory, specifies the type as `sas` and lists the canonical block order used for indexing. No separate manifest file is needed.