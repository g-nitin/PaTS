{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7478e0e2-b7af-4fd4-b44e-ca58e0c31b71",
   "metadata": {},
   "source": [
    "# Getting started with TinyTimeMixer (TTM)\n",
    "\n",
    "This notebooke demonstrates the usage of a pre-trained `TinyTimeMixer` model for several multivariate time series forecasting tasks. For details related to model architecture, refer to the [TTM paper](https://arxiv.org/pdf/2401.03955.pdf).\n",
    "\n",
    "In this example, we will use a pre-trained TTM-512-96 model. That means the TTM model can take an input of 512 time points (`context_length`), and can forecast upto 96 time points (`forecast_length`) in the future. We will use the pre-trained TTM in two settings:\n",
    "1. **Zero-shot**: The pre-trained TTM will be directly used to evaluate on the `test` split of the target data. Note that the TTM was NOT pre-trained on the target data.\n",
    "2. **Few-shot**: The pre-trained TTM will be quickly fine-tuned on only 5% of the `train` split of the target data, and subsequently, evaluated on the `test` part of the target data.\n",
    "\n",
    "Note: Alternatively, this notebook can be modified to try any other TTM model from a suite of TTM models. For details, visit the [Hugging Face TTM Model Repository](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2).\n",
    "\n",
    "1. IBM Granite TTM-R1 pre-trained models can be found here: [Granite-TTM-R1 Model Card](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1)\n",
    "2. IBM Granite TTM-R2 pre-trained models can be found here: [Granite-TTM-R2 Model Card](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2)\n",
    "3. Research-use (non-commercial use only) TTM-R2 pre-trained models can be found here: [Research-Use-TTM-R2](https://huggingface.co/ibm-research/ttm-research-r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab69d3f-a4e4-427a-8c8d-36c82b305855",
   "metadata": {},
   "source": [
    "## Install `tsfm` \n",
    "**[Optional for Local Run / Mandatory for Google Colab]**  \n",
    "Run the below cell to install `tsfm`. Skip if already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c23095-302a-4412-8fc9-a4143ca4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.14\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f0358-1b55-4f45-b1e0-57d2c3e5d904",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63ae353-96df-4380-89f6-1e6cebf684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "\n",
    "from tsfm_public import TimeSeriesPreprocessor, TrackingCallback, count_parameters, get_datasets\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.visualization import plot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894ac389-94e4-4956-8d09-6509d9d452e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "092f5fa8-7f21-46d5-8356-2f313276d345",
   "metadata": {},
   "source": [
    "### Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a826c4f3-1c6c-4088-b6af-f430f45fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# TTM Model path. The default model path is Granite-R2. Below, you can choose other TTM releases.\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "# TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
    "# TTM_MODEL_PATH = \"ibm-research/ttm-research-r2\"\n",
    "\n",
    "# Context length, Or Length of the history.\n",
    "# Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "# Granite-TTM-R2 supports forecast length upto 720 and Granite-TTM-R1 supports forecast length upto 96\n",
    "PREDICTION_LENGTH = 96\n",
    "\n",
    "TARGET_DATASET = \"etth1\"\n",
    "dataset_path = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
    "\n",
    "\n",
    "# Results dir\n",
    "OUT_DIR = \"ttm_finetuned_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d594a0-0f2e-4a3a-b998-96d8d0e6b017",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a56a0cca-1722-4a53-be74-f413841693e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "TARGET_DATASET = \"etth1\"\n",
    "dataset_path = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
    "timestamp_column = \"date\"\n",
    "id_columns = []  # mention the ids that uniquely identify a time-series.\n",
    "\n",
    "target_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "split_config = {\n",
    "    \"train\": [0, 8640],\n",
    "    \"valid\": [8640, 11520],\n",
    "    \"test\": [\n",
    "        11520,\n",
    "        14400,\n",
    "    ],\n",
    "}\n",
    "# Understanding the split config -- slides\n",
    "\n",
    "data = pd.read_csv(\n",
    "    dataset_path,\n",
    "    parse_dates=[timestamp_column],\n",
    ")\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": timestamp_column,\n",
    "    \"id_columns\": id_columns,\n",
    "    \"target_columns\": target_columns,\n",
    "    \"control_columns\": [],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9498749",
   "metadata": {},
   "source": [
    "## Zero-shot evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7935d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_eval(dataset_name, batch_size, context_length=512, forecast_length=96):\n",
    "    # Get data\n",
    "\n",
    "    tsp = TimeSeriesPreprocessor(\n",
    "        **column_specifiers,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_length,\n",
    "        scaling=True,\n",
    "        encode_categorical=False,\n",
    "        scaler_type=\"standard\",\n",
    "    )\n",
    "\n",
    "    dset_train, dset_valid, dset_test = get_datasets(tsp, data, split_config)\n",
    "\n",
    "    # Load model\n",
    "    zeroshot_model = get_model(TTM_MODEL_PATH, context_length=context_length, prediction_length=forecast_length)\n",
    "\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=temp_dir,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "    )\n",
    "    # evaluate = zero-shot performance\n",
    "    print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    print(zeroshot_output)\n",
    "\n",
    "    # get predictions\n",
    "\n",
    "    predictions_dict = zeroshot_trainer.predict(dset_test)\n",
    "\n",
    "    predictions_np = predictions_dict.predictions[0]\n",
    "\n",
    "    print(predictions_np.shape)\n",
    "\n",
    "    # get backbone embeddings (if needed for further analysis)\n",
    "\n",
    "    backbone_embedding = predictions_dict.predictions[1]\n",
    "\n",
    "    print(backbone_embedding.shape)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=zeroshot_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_zeroshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b240d-69cb-4a57-8435-5c7e4de2fc4c",
   "metadata": {},
   "source": [
    "# Zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c54aa-4969-4be3-9d8b-e8b75dfc4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_eval(\n",
    "    dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, forecast_length=PREDICTION_LENGTH, batch_size=64\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b79e3b8-b80a-48c4-9ee7-810e9ebdfcd2",
   "metadata": {},
   "source": [
    " ## Few-shot finetune and evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078c945a-9da7-4729-a95d-43cd615d0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_finetune_eval(\n",
    "    dataset_name,\n",
    "    batch_size,\n",
    "    learning_rate=None,\n",
    "    context_length=512,\n",
    "    forecast_length=96,\n",
    "    fewshot_percent=5,\n",
    "    freeze_backbone=True,\n",
    "    num_epochs=50,\n",
    "    save_dir=OUT_DIR,\n",
    "    loss=\"mse\",\n",
    "    quantile=0.5,\n",
    "):\n",
    "    out_dir = os.path.join(save_dir, dataset_name)\n",
    "\n",
    "    print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "\n",
    "    tsp = TimeSeriesPreprocessor(\n",
    "        **column_specifiers,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_length,\n",
    "        scaling=True,\n",
    "        encode_categorical=False,\n",
    "        scaler_type=\"standard\",\n",
    "    )\n",
    "\n",
    "    dset_train, dset_val, dset_test = get_datasets(\n",
    "        tsp, data, split_config, fewshot_fraction=fewshot_percent / 100, fewshot_location=\"first\"\n",
    "    )\n",
    "\n",
    "    # change head dropout to 0.7 for ett datasets\n",
    "    if \"ett\" in dataset_name:\n",
    "        finetune_forecast_model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=context_length,\n",
    "            prediction_length=forecast_length,\n",
    "            head_dropout=0.7,\n",
    "            loss=loss,\n",
    "            # quantile=quantile,\n",
    "        )\n",
    "    else:\n",
    "        finetune_forecast_model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=context_length,\n",
    "            prediction_length=forecast_length,\n",
    "            loss=loss,\n",
    "            quantile=quantile,\n",
    "        )\n",
    "\n",
    "    if freeze_backbone:\n",
    "        print(\n",
    "            \"Number of params before freezing backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "        # Freeze the backbone of the model\n",
    "        for param in finetune_forecast_model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Count params\n",
    "        print(\n",
    "            \"Number of params after freezing the backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "    # Find optimal learning rate\n",
    "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
    "    if learning_rate is None:\n",
    "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
    "            finetune_forecast_model,\n",
    "            dset_train,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
    "\n",
    "    print(f\"Using learning rate = {learning_rate}\")\n",
    "    finetune_forecast_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"output\"),\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        dataloader_num_workers=8,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create the early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
    "    )\n",
    "    tracking_callback = TrackingCallback()\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
    "    )\n",
    "\n",
    "    finetune_forecast_trainer = Trainer(\n",
    "        model=finetune_forecast_model,\n",
    "        args=finetune_forecast_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val,\n",
    "        callbacks=[early_stopping_callback, tracking_callback],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
    "\n",
    "    # Fine tune\n",
    "    finetune_forecast_trainer.train()\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
    "\n",
    "    finetune_forecast_trainer.model.loss = \"mse\"  # fixing metric to mse for evaluation\n",
    "\n",
    "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "    print(fewshot_output)\n",
    "    print(\"+\" * 60)\n",
    "\n",
    "    # get predictions\n",
    "\n",
    "    predictions_dict = finetune_forecast_trainer.predict(dset_test)\n",
    "\n",
    "    predictions_np = predictions_dict.predictions[0]\n",
    "\n",
    "    print(predictions_np.shape)\n",
    "\n",
    "    # get backbone embeddings (if needed for further analysis)\n",
    "\n",
    "    backbone_embedding = predictions_dict.predictions[1]\n",
    "\n",
    "    print(backbone_embedding.shape)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=finetune_forecast_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_fewshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e825cf28-f034-4a32-a729-0fe846ff2a26",
   "metadata": {},
   "source": [
    "### Few-shot 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145fead-50fb-4e3e-89fc-a0c238755e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_length=PREDICTION_LENGTH,\n",
    "    batch_size=64,\n",
    "    fewshot_percent=5,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e561f-ff9f-4817-95f8-2721668fc2af",
   "metadata": {},
   "source": [
    "# Fewshot with quantile loss (We can use pinball loss to generate different quantiles as required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f92363-761e-41ea-bf48-d1c27b3376ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_length=PREDICTION_LENGTH,\n",
    "    batch_size=64,\n",
    "    fewshot_percent=5,\n",
    "    learning_rate=0.001,\n",
    "    loss=\"pinball\",\n",
    "    quantile=0.5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee2dcc1-bcb8-47ee-8ba7-ff3104159ed6",
   "metadata": {},
   "source": [
    "## Example: TTM for other forecast horizon lengths and context lengths\n",
    "\n",
    "\n",
    "**Context length, Or Length of the history:** Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
    "\n",
    "**Forecast length, Or prediction length:** Granite-TTM-R2 supports forecast length upto 720 and Granite-TTM-R1 supports forecast length upto 96\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31b58250-91a2-4db0-ab45-29b89f5afd0e",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eff2e1-acfd-4c5b-8463-e084ba831cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_eval(dataset_name=TARGET_DATASET, context_length=1024, forecast_length=48, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00e56d63-5e87-41ae-8fe9-3b3cfd9d19f6",
   "metadata": {},
   "source": [
    "### Few-shot 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56cd24-bae6-4cc6-9a3c-52f965014eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    context_length=1536,\n",
    "    forecast_length=48,\n",
    "    batch_size=64,\n",
    "    fewshot_percent=5,\n",
    "    learning_rate=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsplans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
